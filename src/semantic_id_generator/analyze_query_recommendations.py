"""
åˆ†ææŸ¥è¯¢æ¨èç»“æœ     
ä»semantic_query_vote_doc.csvåŠ è½½æŸ¥è¯¢æ•°æ®ï¼Œå¯¹æ¯ä¸ªqueryä½¿ç”¨æ¡£ä½0æˆ–-1çš„æ­Œæ›²ï¼Œ
åœ¨æ­Œæ›²å‘é‡ä¸­æŸ¥æ‰¾è·ç¦»æœ€è¿‘çš„å‰20ä¸ªæ­Œæ›²ï¼Œå¹¶å±•ç¤ºå¯¹æ¯”ç»“æœã€‚
"""

import os
import sys
import json
import csv
import numpy as np
import argparse
import pickle
from typing import Dict, List, Tuple
from tqdm import tqdm
from collections import defaultdict

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("è­¦å‘Š: faiss æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è¾ƒæ…¢çš„numpyè®¡ç®—ã€‚å»ºè®®å®‰è£…: pip install faiss-cpu")

# Add project root to sys.path to allow for absolute imports
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import Config


class QueryRecommendationAnalyzer:
    def __init__(self, config: Config, query_vote_file: str, use_faiss: bool = True):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            query_vote_file: semantic_query_vote_doc.csv æ–‡ä»¶è·¯å¾„
            use_faiss: æ˜¯å¦ä½¿ç”¨FAISSåŠ é€Ÿæ£€ç´¢
        """
        print("--- æŸ¥è¯¢æ¨èåˆ†æå·¥å…· ---")
        self.config = config
        self.query_vote_file = query_vote_file
        self.use_faiss = use_faiss and FAISS_AVAILABLE
        
        # FAISSç´¢å¼•ç›¸å…³
        self.faiss_index = None
        self.song_id_list = None  # ä¸FAISSç´¢å¼•å¯¹åº”çš„song_idåˆ—è¡¨
        self.index_cache_path = os.path.join(config.output_dir, "semantic_id", "faiss_index.bin")
        self.id_list_cache_path = os.path.join(config.output_dir, "semantic_id", "faiss_song_ids.pkl")
        self.index_meta_path = os.path.join(config.output_dir, "semantic_id", "faiss_index_meta.json")
        
        # åŠ è½½æ•°æ®
        self.song_info = self._load_song_info(config.data.song_info_file)
        self.song_vectors = self._load_song_vectors(config.data.song_vectors_file)
        self.semantic_ids = self._load_semantic_ids(config.data.semantic_ids_file)
        self.query_data = self._load_query_vote_data(query_vote_file)
        
        if not self.song_info or not self.song_vectors or not self.semantic_ids:
            raise RuntimeError("åŠ è½½å¿…è¦æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config.py ä¸­çš„è·¯å¾„é…ç½®ã€‚")
        
        if not self.query_data:
            raise RuntimeError(f"åŠ è½½æŸ¥è¯¢æ•°æ®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶: {query_vote_file}")
        
        # æ„å»ºæˆ–åŠ è½½FAISSç´¢å¼•
        if self.use_faiss:
            self._build_or_load_faiss_index()
        
        print("\nåˆå§‹åŒ–å®Œæˆï¼Œåˆ†æå™¨å·²å°±ç»ªã€‚\n")
    
    def _load_song_info(self, path: str) -> Dict[str, Dict[str, str]]:
        """
        åŠ è½½æ­Œæ›²ä¿¡æ¯
        æ ¼å¼: song_id\tsong_name\tsinger
        """
        print(f"åŠ è½½æ­Œæ›²ä¿¡æ¯: {path}")
        info = {}
        try:
            with open(path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                for row in tqdm(reader, desc="è¯»å–æ­Œæ›²ä¿¡æ¯"):
                    if len(row) >= 3:
                        info[row[0]] = {"name": row[1], "singer": row[2]}
            print(f"  âœ“ åŠ è½½äº† {len(info)} é¦–æ­Œæ›²ä¿¡æ¯")
        except FileNotFoundError:
            print(f"  âš ï¸  æ­Œæ›²ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        return info
    
    def _load_song_vectors(self, path: str) -> Dict[str, np.ndarray]:
        """
        åŠ è½½æ­Œæ›²å‘é‡
        æ ¼å¼: song_id,vec1,vec2,...,vecN
        """
        print(f"åŠ è½½æ­Œæ›²å‘é‡: {path}")
        vectors = {}
        try:
            with open(path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in tqdm(reader, desc="è¯»å–æ­Œæ›²å‘é‡"):
                    if len(row) > 1:
                        vectors[row[0]] = np.array(row[1:], dtype=np.float32)
            print(f"  âœ“ åŠ è½½äº† {len(vectors)} ä¸ªæ­Œæ›²å‘é‡ (ç»´åº¦: {len(next(iter(vectors.values())))} )")
        except FileNotFoundError:
            print(f"  âš ï¸  æ­Œæ›²å‘é‡æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        return vectors
    
    def _load_semantic_ids(self, path: str) -> Dict[str, Tuple]:
        """
        åŠ è½½è¯­ä¹‰ID
        æ ¼å¼: {"song_id": "xxx", "semantic_ids": [1, 2, 3]}
        """
        print(f"åŠ è½½è¯­ä¹‰ID: {path}")
        s_ids = {}
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in tqdm(f, desc="è¯»å–è¯­ä¹‰ID"):
                    item = json.loads(line)
                    song_id = item['song_id']
                    sem_id_tuple = tuple(item['semantic_ids'])
                    s_ids[song_id] = sem_id_tuple
            print(f"  âœ“ åŠ è½½äº† {len(s_ids)} ä¸ªè¯­ä¹‰ID")
        except FileNotFoundError:
            print(f"  âš ï¸  è¯­ä¹‰IDæ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        return s_ids
    
    def _load_query_vote_data(self, path: str) -> List[Dict]:
        """
        åŠ è½½æŸ¥è¯¢æŠ•ç¥¨æ•°æ®
        æ ¼å¼: query,song_infos,search_pv,cnt (CSVæ ¼å¼ï¼Œé€—å·åˆ†éš”)
        song_infosæ ¼å¼: song_id:gear@@song_id:gear@@...
        """
        print(f"åŠ è½½æŸ¥è¯¢æ•°æ®: {path}")
        data = []
        
        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb18030', 'utf-8-sig', 'latin1']
        
        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding) as f:
                    # ä½¿ç”¨é€—å·ä½œä¸ºåˆ†éš”ç¬¦ï¼ˆExcelå¯¼å‡ºçš„CSVæ ¼å¼ï¼‰
                    reader = csv.DictReader(f, delimiter=',')
                    data = []
                    for row in tqdm(reader, desc=f"è¯»å–æŸ¥è¯¢æ•°æ® (ç¼–ç : {encoding})"):
                        # æ£€æŸ¥å¿…éœ€çš„å­—æ®µæ˜¯å¦å­˜åœ¨
                        if 'query' in row and 'song_infos' in row:
                            data.append({
                                'query': row['query'].strip(),
                                'song_infos': row['song_infos'].strip(),
                                'search_pv': row.get('search_pv', '').strip(),
                                'cnt': row.get('cnt', '').strip()
                            })
                
                if data:
                    print(f"  âœ“ ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸåŠ è½½äº† {len(data)} æ¡æŸ¥è¯¢è®°å½•")
                    break
                else:
                    print(f"  âš ï¸  ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æˆåŠŸï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                    
            except UnicodeDecodeError:
                if encoding == encodings[-1]:
                    print(f"  âš ï¸  å°è¯•äº†æ‰€æœ‰ç¼–ç  {encodings} éƒ½å¤±è´¥")
                    print(f"  ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è½¬æ¢æ–‡ä»¶ç¼–ç ï¼š")
                    print(f"     iconv -f GBK -t UTF-8 {path} > {path}.utf8")
                continue
            except FileNotFoundError:
                print(f"  âš ï¸  æŸ¥è¯¢æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
                break
            except KeyError as e:
                print(f"  âš ï¸  CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {e}")
                print(f"  ğŸ’¡ æœŸæœ›çš„åˆ—å: query, song_infos, search_pv, cnt")
                break
            except Exception as e:
                print(f"  âš ï¸  åŠ è½½æŸ¥è¯¢æ•°æ®æ—¶å‡ºé”™ (ç¼–ç : {encoding}): {e}")
                if encoding == encodings[-1]:
                    break
                continue
        
        return data
    
    def parse_song_infos(self, song_infos_str: str, top_n: int = 20) -> List[Tuple[str, int]]:
        """
        è§£æsong_infoså­—ç¬¦ä¸²
        
        Args:
            song_infos_str: æ ¼å¼å¦‚ "32189764:-1@@27709143:-1@@111055831:-1..."
            top_n: æå–å‰Nä¸ªæ­Œæ›²
            
        Returns:
            [(song_id, gear), ...] åˆ—è¡¨
        """
        songs = []
        items = song_infos_str.split('@@')
        
        for item in items[:top_n]:
            if ':' in item:
                parts = item.split(':')
                if len(parts) >= 2:
                    song_id = parts[0].strip()
                    try:
                        gear = int(parts[1])
                        songs.append((song_id, gear))
                    except ValueError:
                        continue
        
        return songs
    
    def get_seed_songs(self, songs: List[Tuple[str, int]]) -> List[str]:
        """
        è·å–æ¡£ä½ä¸º0æˆ–-1çš„æ­Œæ›²ä½œä¸ºç§å­æ­Œæ›²
        
        Args:
            songs: [(song_id, gear), ...] åˆ—è¡¨
            
        Returns:
            ç§å­æ­Œæ›²IDåˆ—è¡¨
        """
        seed_songs = []
        for song_id, gear in songs:
            if gear in [0, -1]:
                seed_songs.append(song_id)
        return seed_songs
    
    def _build_or_load_faiss_index(self):
        """
        æ„å»ºæˆ–åŠ è½½FAISSç´¢å¼•ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„å‘é‡æ–‡ä»¶
        """
        vectors_file = self.config.data.song_vectors_file
        need_rebuild = False
        
        # æ£€æŸ¥å‘é‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(vectors_file):
            raise FileNotFoundError(f"å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {vectors_file}")
        
        # è·å–å‘é‡æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        vectors_mtime = os.path.getmtime(vectors_file)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„ç´¢å¼•åŠå…ƒæ•°æ®
        if (os.path.exists(self.index_cache_path) and 
            os.path.exists(self.id_list_cache_path) and 
            os.path.exists(self.index_meta_path)):
            
            # è¯»å–å…ƒæ•°æ®
            try:
                with open(self.index_meta_path, 'r') as f:
                    meta = json.load(f)
                
                cached_mtime = meta.get('vectors_mtime', 0)
                cached_file = meta.get('vectors_file', '')
                
                # æ£€æŸ¥å‘é‡æ–‡ä»¶æ˜¯å¦æ›´æ–°
                if cached_file != vectors_file:
                    print(f"\nâš ï¸  å‘é‡æ–‡ä»¶è·¯å¾„å·²å˜æ›´: {cached_file} -> {vectors_file}")
                    need_rebuild = True
                elif vectors_mtime > cached_mtime:
                    print(f"\nâš ï¸  å‘é‡æ–‡ä»¶å·²æ›´æ–°ï¼ˆç¼“å­˜æ—¶é—´: {cached_mtime}, å½“å‰æ—¶é—´: {vectors_mtime}ï¼‰")
                    need_rebuild = True
                else:
                    # å°è¯•åŠ è½½ç¼“å­˜çš„ç´¢å¼•
                    print(f"\nåŠ è½½å·²ç¼“å­˜çš„FAISSç´¢å¼•: {self.index_cache_path}")
                    try:
                        self.faiss_index = faiss.read_index(self.index_cache_path)
                        with open(self.id_list_cache_path, 'rb') as f:
                            self.song_id_list = pickle.load(f)
                        print(f"  âœ“ æˆåŠŸåŠ è½½FAISSç´¢å¼•ï¼ŒåŒ…å« {self.faiss_index.ntotal} ä¸ªå‘é‡")
                        print(f"  âœ“ ç´¢å¼•åŸºäºå‘é‡æ–‡ä»¶: {cached_file}")
                        return
                    except Exception as e:
                        print(f"  âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                        need_rebuild = True
            except Exception as e:
                print(f"\nâš ï¸  è¯»å–ç´¢å¼•å…ƒæ•°æ®å¤±è´¥: {e}")
                need_rebuild = True
        else:
            print("\næœªæ‰¾åˆ°ç¼“å­˜çš„FAISSç´¢å¼•")
            need_rebuild = True
        
        # æ„å»ºæ–°ç´¢å¼•
        if need_rebuild:
            print("\næ„å»ºFAISSç´¢å¼•...")
            song_ids = list(self.song_vectors.keys())
            vectors = np.array([self.song_vectors[sid] for sid in song_ids], dtype=np.float32)
            
            print(f"  - æ­Œæ›²æ•°é‡: {len(song_ids)}")
            print(f"  - å‘é‡ç»´åº¦: {vectors.shape[1]}")
            
            # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            faiss.normalize_L2(vectors)
            
            # åˆ›å»ºç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ï¼Œå› ä¸ºå‘é‡å·²å½’ä¸€åŒ–ï¼Œå†…ç§¯ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            dimension = vectors.shape[1]
            index = faiss.IndexFlatIP(dimension)  # Inner Product (ä½™å¼¦ç›¸ä¼¼åº¦)
            index.add(vectors)
            
            self.faiss_index = index
            self.song_id_list = song_ids
            
            print(f"  âœ“ FAISSç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡")
            
            # ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜
            print(f"\nä¿å­˜FAISSç´¢å¼•åˆ°: {self.index_cache_path}")
            try:
                os.makedirs(os.path.dirname(self.index_cache_path), exist_ok=True)
                
                # ä¿å­˜ç´¢å¼•æ–‡ä»¶
                faiss.write_index(self.faiss_index, self.index_cache_path)
                
                # ä¿å­˜æ­Œæ›²IDåˆ—è¡¨
                with open(self.id_list_cache_path, 'wb') as f:
                    pickle.dump(self.song_id_list, f)
                
                # ä¿å­˜å…ƒæ•°æ®ï¼ˆåŒ…å«å‘é‡æ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼‰
                meta = {
                    'vectors_file': vectors_file,
                    'vectors_mtime': vectors_mtime,
                    'num_vectors': len(song_ids),
                    'dimension': dimension,
                    'created_at': os.path.getmtime(self.index_cache_path)
                }
                with open(self.index_meta_path, 'w') as f:
                    json.dump(meta, f, indent=2)
                
                print("  âœ“ ç´¢å¼•åŠå…ƒæ•°æ®ä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"  âš ï¸  ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def calculate_average_vector(self, song_ids: List[str]) -> np.ndarray:
        """
        è®¡ç®—å¤šä¸ªæ­Œæ›²çš„å¹³å‡å‘é‡
        
        Args:
            song_ids: æ­Œæ›²IDåˆ—è¡¨
            
        Returns:
            å¹³å‡å‘é‡ï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆå‘é‡åˆ™è¿”å›None
        """
        vectors = []
        for song_id in song_ids:
            if song_id in self.song_vectors:
                vectors.append(self.song_vectors[song_id])
        
        if not vectors:
            return None
        
        return np.mean(vectors, axis=0)
    
    def find_nearest_songs(
        self,
        query_vector: np.ndarray,
        exclude_ids: List[str],
        top_n: int = 20
    ) -> List[Tuple[str, float]]:
        """
        æ‰¾åˆ°ä¸æŸ¥è¯¢å‘é‡æœ€è¿‘çš„æ­Œæ›²ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            exclude_ids: è¦æ’é™¤çš„æ­Œæ›²IDåˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªæœ€è¿‘çš„æ­Œæ›²
            
        Returns:
            [(song_id, similarity), ...] åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if self.use_faiss and self.faiss_index is not None:
            return self._find_nearest_songs_faiss(query_vector, exclude_ids, top_n)
        else:
            return self._find_nearest_songs_numpy(query_vector, exclude_ids, top_n)
    
    def _find_nearest_songs_faiss(
        self,
        query_vector: np.ndarray,
        exclude_ids: List[str],
        top_n: int = 20
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨FAISSæ‰¾åˆ°æœ€è¿‘çš„æ­Œæ›²
        """
        exclude_set = set(exclude_ids)
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_vec = query_vector.copy().reshape(1, -1).astype(np.float32)
        faiss.normalize_L2(query_vec)
        
        # æœç´¢æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤æ’é™¤çš„æ­Œæ›²
        k = min(top_n + len(exclude_ids) + 100, self.faiss_index.ntotal)
        distances, indices = self.faiss_index.search(query_vec, k)
        
        # è¿‡æ»¤æ’é™¤çš„æ­Œæ›²å¹¶è¿”å›top_n
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            song_id = self.song_id_list[idx]
            if song_id not in exclude_set:
                results.append((song_id, float(dist)))
                if len(results) >= top_n:
                    break
        
        return results
    
    def _find_nearest_songs_numpy(
        self,
        query_vector: np.ndarray,
        exclude_ids: List[str],
        top_n: int = 20
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨numpyè®¡ç®—æœ€è¿‘çš„æ­Œæ›²ï¼ˆè¾ƒæ…¢ï¼Œç”¨äºæ²¡æœ‰FAISSçš„æƒ…å†µï¼‰
        """
        similarities = []
        exclude_set = set(exclude_ids)
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_vector_norm = query_vector / np.linalg.norm(query_vector)
        
        for song_id, vector in self.song_vectors.items():
            if song_id not in exclude_set:
                # å½’ä¸€åŒ–æ­Œæ›²å‘é‡å¹¶è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                vector_norm = vector / np.linalg.norm(vector)
                cosine_sim = np.dot(query_vector_norm, vector_norm)
                similarities.append((song_id, float(cosine_sim)))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºå¹¶è¿”å›å‰Nä¸ª
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_n]
    
    def get_song_display_info(self, song_id: str) -> str:
        """
        è·å–æ­Œæ›²çš„å±•ç¤ºä¿¡æ¯
        
        Args:
            song_id: æ­Œæ›²ID
            
        Returns:
            æ ¼å¼åŒ–çš„æ­Œæ›²ä¿¡æ¯å­—ç¬¦ä¸²
        """
        info = self.song_info.get(song_id, {"name": "æœªçŸ¥", "singer": "æœªçŸ¥"})
        semantic_id = self.semantic_ids.get(song_id, ())
        semantic_id_str = str(semantic_id) if semantic_id else "N/A"
        return f"ID:{song_id} | {info['name']} - {info['singer']} | è¯­ä¹‰ID:{semantic_id_str}"
    
    def analyze_query(self, query_data: Dict, query_idx: int = 0):
        """
        åˆ†æå•ä¸ªæŸ¥è¯¢
        
        Args:
            query_data: æŸ¥è¯¢æ•°æ®å­—å…¸
            query_idx: æŸ¥è¯¢ç´¢å¼•ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        """
        query = query_data['query']
        song_infos_str = query_data['song_infos']
        
        # 1. è§£æå‰20ä¸ªæ­Œæ›²åŠå…¶æ¡£ä½
        top_20_songs = self.parse_song_infos(song_infos_str, top_n=20)
        
        # 2. è·å–ç§å­æ­Œæ›²ï¼ˆæ¡£ä½0æˆ–-1ï¼‰
        seed_songs = self.get_seed_songs(top_20_songs)
        
        # 3. è®¡ç®—ç§å­æ­Œæ›²çš„å¹³å‡å‘é‡å¹¶æ£€ç´¢
        nearest_songs = []
        if seed_songs:
            query_vector = self.calculate_average_vector(seed_songs)
            if query_vector is not None:
                exclude_ids = [song_id for song_id, _ in top_20_songs]
                nearest_songs = self.find_nearest_songs(query_vector, exclude_ids, top_n=20)
        
        # 4. å¹¶åˆ—å±•ç¤ºç»“æœ
        self._display_side_by_side(query, query_idx, top_20_songs, seed_songs, nearest_songs)
    
    def _display_side_by_side(
        self, 
        query: str, 
        query_idx: int, 
        original_songs: List[Tuple[str, int]], 
        seed_songs: List[str],
        vector_songs: List[Tuple[str, float]]
    ):
        """
        å¹¶åˆ—å±•ç¤ºåŸå§‹æ¨èå’Œå‘é‡æ£€ç´¢ç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            query_idx: æŸ¥è¯¢ç´¢å¼•
            original_songs: åŸå§‹æ¨èæ­Œæ›²åˆ—è¡¨ [(song_id, gear), ...]
            seed_songs: ç§å­æ­Œæ›²IDåˆ—è¡¨
            vector_songs: å‘é‡æ£€ç´¢æ­Œæ›²åˆ—è¡¨ [(song_id, similarity), ...]
        """
        # æ‰“å°æŸ¥è¯¢
        print("\n" + "=" * 220)
        print(f"æŸ¥è¯¢ #{query_idx + 1}: {query} (ç§å­æ­Œæ›²æ•°: {len(seed_songs)})")
        print("=" * 220)
        
        # è¡¨å¤´
        left_header = "ã€åŸå§‹æ¨è Top20ã€‘"
        right_header = "ã€å‘é‡æ£€ç´¢ Top20ã€‘"
        print(f"\n{left_header:<100} | {right_header}")
        print("-" * 100 + " | " + "-" * 100)
        
        # å¹¶åˆ—å±•ç¤º20è¡Œ
        for i in range(20):
            # å·¦ä¾§ï¼šåŸå§‹æ¨è
            if i < len(original_songs):
                song_id, gear = original_songs[i]
                info = self.song_info.get(song_id, {"name": "æœªçŸ¥", "singer": "æœªçŸ¥"})
                semantic_id = self.semantic_ids.get(song_id, ())
                sem_str = str(semantic_id) if semantic_id else "N/A"
                
                # æ ‡è®°ç§å­æ­Œæ›²
                marker = "âœ“" if gear in [0, -1] else " "
                left_line = f"{i+1:2d}.[æ¡£{gear:2d}]{marker} ID:{song_id:<10} {info['name'][:12]:<12} - {info['singer'][:8]:<8} {sem_str}"
            else:
                left_line = ""
            
            # å³ä¾§ï¼šå‘é‡æ£€ç´¢
            if i < len(vector_songs):
                song_id, similarity = vector_songs[i]
                info = self.song_info.get(song_id, {"name": "æœªçŸ¥", "singer": "æœªçŸ¥"})
                semantic_id = self.semantic_ids.get(song_id, ())
                sem_str = str(semantic_id) if semantic_id else "N/A"
                
                right_line = f"{i+1:2d}.[{similarity:.3f}] ID:{song_id:<10} {info['name'][:12]:<12} - {info['singer'][:8]:<8} {sem_str}"
            else:
                right_line = ""
            
            # æ‰“å°ä¸€è¡Œ
            print(f"{left_line:<100} | {right_line}")
        
        print()
    
    def analyze_all_queries(self, max_queries: int = None):
        """
        åˆ†ææ‰€æœ‰æŸ¥è¯¢
        
        Args:
            max_queries: æœ€å¤šåˆ†æçš„æŸ¥è¯¢æ•°é‡ï¼ŒNoneè¡¨ç¤ºåˆ†ææ‰€æœ‰
        """
        total_queries = len(self.query_data)
        if max_queries:
            total_queries = min(total_queries, max_queries)
        
        print(f"\nå¼€å§‹åˆ†æ {total_queries} ä¸ªæŸ¥è¯¢...\n")
        
        for idx in range(total_queries):
            self.analyze_query(self.query_data[idx], idx)
    
    def run(self, max_queries: int = None):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            max_queries: æœ€å¤šåˆ†æçš„æŸ¥è¯¢æ•°é‡
        """
        self.analyze_all_queries(max_queries)


def main():
    parser = argparse.ArgumentParser(description='åˆ†ææŸ¥è¯¢æ¨èç»“æœ')
    parser.add_argument(
        '--query_vote',
        type=str,
        default='semantic_query_vote_doc.csv',
        help='æŸ¥è¯¢æŠ•ç¥¨æ•°æ®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--max_queries',
        type=int,
        default=None,
        help='æœ€å¤šåˆ†æçš„æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤åˆ†ææ‰€æœ‰ï¼‰'
    )
    parser.add_argument(
        '--no_faiss',
        action='store_true',
        help='ä¸ä½¿ç”¨FAISSåŠ é€Ÿï¼ˆä½¿ç”¨numpyè®¡ç®—ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = QueryRecommendationAnalyzer(
        config=config,
        query_vote_file=args.query_vote,
        use_faiss=not args.no_faiss
    )
    
    analyzer.run(max_queries=args.max_queries)


if __name__ == '__main__':
    main()

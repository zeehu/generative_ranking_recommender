"""
é¢„å¤„ç†è„šæœ¬: æé€Ÿå¤šè¿›ç¨‹ç‰ˆæœ¬ - å……åˆ†åˆ©ç”¨20æ ¸CPU + 50GBå†…å­˜ 
é¢„è®¡é€Ÿåº¦: 10,000-15,000 samples/s (10-15å€æå‡)
é¢„è®¡æ—¶é—´: 4-7åˆ†é’Ÿ (vs åŸæ¥71åˆ†é’Ÿ)
"""
import os
import sys
import logging
import shutil
from tqdm import tqdm
from datasets import Dataset as HFDataset, concatenate_datasets
import gc
import pyarrow as pa
import pyarrow.parquet as pq
from typing import List, Tuple
from multiprocessing import Pool, cpu_count
import time

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from config_optimized import Config
except ImportError:
    from config import Config
from src.generator.tiger_model import TIGERModel
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šworkerè¿›ç¨‹çš„tokenizer
worker_tokenizer = None


def safe_remove_dir(path: str, max_retries: int = 3, retry_delay: float = 1.0):
    """
    å®‰å…¨åˆ é™¤ç›®å½•ï¼Œå¤„ç†å¯èƒ½çš„æ–‡ä»¶é”å®šé—®é¢˜
    å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰
    
    Args:
        path: è¦åˆ é™¤çš„ç›®å½•è·¯å¾„
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    """
    if not os.path.exists(path):
        logger.debug(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {path}")
        return
    
    for attempt in range(max_retries):
        try:
            shutil.rmtree(path)
            logger.info(f"âœ… å·²åˆ é™¤ç›®å½•: {path}")
            return
        except OSError as e:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ åˆ é™¤ç›®å½•å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å¯èƒ½çš„æ–‡ä»¶å¥æŸ„
            else:
                logger.error(f"âŒ æ— æ³•åˆ é™¤ç›®å½• {path} (å·²é‡è¯• {max_retries} æ¬¡): {e}")
                logger.warning(f"âš ï¸ è¯·æ‰‹åŠ¨åˆ é™¤è¯¥ç›®å½•: {path}")
                raise


def init_worker(model_name: str, layer_vocab_sizes: dict):
    """
    åˆå§‹åŒ–workerè¿›ç¨‹çš„tokenizerï¼ˆæ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªç‹¬ç«‹çš„tokenizerï¼‰
    """
    global worker_tokenizer
    from src.generator.tiger_model import TIGERModel
    model = TIGERModel(base_model=model_name, layer_vocab_sizes=layer_vocab_sizes)
    worker_tokenizer = model.tokenizer


def tokenize_chunk_worker(args: Tuple[List[str], List[str], int, int, int, str]) -> Tuple[str, int]:
    """
    Workerå‡½æ•°ï¼štokenizeä¸€ä¸ªæ•°æ®å—
    
    Args:
        args: (input_texts, target_texts, max_input_len, max_target_len, chunk_id, temp_dir)
    
    Returns:
        (parquet_file_path, num_samples)
    """
    input_texts, target_texts, max_input_len, max_target_len, chunk_id, temp_dir = args
    
    # ä½¿ç”¨workerçš„tokenizer
    global worker_tokenizer
    
    # æ‰¹é‡tokenizeï¼ˆHuggingFace tokenizerå†…éƒ¨å·²ä¼˜åŒ–ï¼‰
    input_encodings = worker_tokenizer.base_tokenizer(
        input_texts,
        max_length=max_input_len,
        truncation=True,
        padding='max_length',
        return_tensors=None
    )
    
    target_encodings = worker_tokenizer.base_tokenizer(
        target_texts,
        max_length=max_target_len,
        truncation=True,
        padding='max_length',
        return_tensors=None
    )
    
    # æ„å»ºArrow Tableï¼ˆé›¶æ‹·è´ï¼‰
    schema = pa.schema([
        ('input_ids', pa.list_(pa.int64())),
        ('attention_mask', pa.list_(pa.int64())),
        ('labels', pa.list_(pa.int64()))
    ])
    
    arrays = [
        pa.array(input_encodings['input_ids']),
        pa.array(input_encodings['attention_mask']),
        pa.array(target_encodings['input_ids'])
    ]
    table = pa.Table.from_arrays(arrays, schema=schema)
    
    # å†™å…¥parquetæ–‡ä»¶ï¼ˆsnappyå‹ç¼©ï¼‰
    parquet_file = os.path.join(temp_dir, f"chunk_{chunk_id:06d}.parquet")
    pq.write_table(table, parquet_file, compression='snappy')
    
    return parquet_file, len(input_texts)


def read_and_split_data(data_path: str, chunk_size: int) -> List[Tuple[List[str], List[str]]]:
    """
    å¿«é€Ÿè¯»å–TSVæ–‡ä»¶å¹¶åˆ†å‰²æˆå¤šä¸ªchunkï¼ˆç”¨äºå¤šè¿›ç¨‹å¤„ç†ï¼‰
    
    Args:
        data_path: TSVæ–‡ä»¶è·¯å¾„
        chunk_size: æ¯ä¸ªchunkçš„å¤§å°
    
    Returns:
        List of (input_texts, target_texts) tuples
    """
    logger.info(f"è¯»å–æ–‡ä»¶: {data_path}")
    logger.info(f"Chunkå¤§å°: {chunk_size:,} æ ·æœ¬/chunk")
    
    chunks = []
    input_texts = []
    target_texts = []
    total_lines = 0
    
    # ä½¿ç”¨å¤§ç¼“å†²åŒºåŠ é€Ÿè¯»å–
    with open(data_path, 'r', encoding='utf-8', buffering=32*1024*1024) as f:  # 32MBç¼“å†²
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) != 3:
                continue
            
            glid, input_text, target_text = parts
            input_texts.append(input_text)
            target_texts.append(target_text)
            total_lines += 1
            
            if len(input_texts) >= chunk_size:
                chunks.append((input_texts, target_texts))
                input_texts = []
                target_texts = []
    
    # æ·»åŠ å‰©ä½™æ•°æ®
    if input_texts:
        chunks.append((input_texts, target_texts))
    
    logger.info(f"æ€»æ ·æœ¬æ•°: {total_lines:,}")
    logger.info(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªchunks")
    logger.info(f"é¢„è®¡æ¯ä¸ªè¿›ç¨‹å¤„ç† {len(chunks)//cpu_count()} ä¸ªchunks")
    
    return chunks


def tokenize_dataset_multiproc(
    data_path: str,
    model_name: str,
    layer_vocab_sizes: dict,
    max_input_len: int,
    max_target_len: int,
    output_path: str,
    chunk_size: int = 20000,
    num_proc: int = None
):
    """
    å¤šè¿›ç¨‹å¹¶è¡Œtokenize - å……åˆ†åˆ©ç”¨20æ ¸CPU + 50GBå†…å­˜
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. å¤šè¿›ç¨‹å¹¶è¡Œtokenizationï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹tokenizerï¼‰
    2. å¤§chunk sizeï¼ˆ20000æ ·æœ¬/chunkï¼Œå……åˆ†åˆ©ç”¨å†…å­˜ï¼‰
    3. åˆ†æ‰¹åˆå¹¶parquetï¼ˆé¿å…OOMï¼‰
    4. Arrowé›¶æ‹·è´ + Snappyå‹ç¼©
    
    Args:
        data_path: TSVæ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        layer_vocab_sizes: å±‚çº§è¯è¡¨å¤§å°
        max_input_len: è¾“å…¥æœ€å¤§é•¿åº¦
        max_target_len: ç›®æ ‡æœ€å¤§é•¿åº¦
        output_path: è¾“å‡ºè·¯å¾„
        chunk_size: æ¯ä¸ªchunkçš„æ ·æœ¬æ•°ï¼ˆå»ºè®®20000-50000ï¼‰
        num_proc: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤CPUæ ¸å¿ƒæ•°-2ï¼‰
    """
    if num_proc is None:
        num_proc = max(1, cpu_count() - 2)
    
    logger.info("=" * 80)
    logger.info("ğŸš€ å¤šè¿›ç¨‹å¹¶è¡ŒTokenizationï¼ˆæé€Ÿç‰ˆæœ¬ï¼‰")
    logger.info("=" * 80)
    logger.info(f"æ–‡ä»¶è·¯å¾„: {data_path}")
    logger.info(f"Chunkå¤§å°: {chunk_size:,} æ ·æœ¬/chunk")
    logger.info(f"å¹¶è¡Œè¿›ç¨‹æ•°: {num_proc} (CPUæ ¸å¿ƒæ•°: {cpu_count()})")
    logger.info(f"é¢„è®¡é€Ÿåº¦: 10,000-15,000 samples/s (vs åŸæ¥915 samples/s)")
    logger.info(f"é¢„è®¡æå‡: 10-15å€")
    
    start_time = time.time()
    
    # å‡†å¤‡ä¸´æ—¶ç›®å½•
    temp_parquet_dir = output_path + "_temp_parquet"
    safe_remove_dir(temp_parquet_dir)
    os.makedirs(temp_parquet_dir, exist_ok=True)
    
    # æ­¥éª¤1: å¿«é€Ÿè¯»å–å¹¶åˆ†å‰²æ•°æ®
    logger.info("\nğŸ“– æ­¥éª¤1: è¯»å–å¹¶åˆ†å‰²æ•°æ®...")
    read_start = time.time()
    chunks = read_and_split_data(data_path, chunk_size)
    read_time = time.time() - read_start
    logger.info(f"âœ… è¯»å–å®Œæˆï¼Œè€—æ—¶: {read_time:.1f}ç§’")
    
    # æ­¥éª¤2: å¤šè¿›ç¨‹å¹¶è¡Œtokenization
    logger.info(f"\nâš¡ æ­¥éª¤2: å¯åŠ¨ {num_proc} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œtokenization...")
    
    # å‡†å¤‡å‚æ•°
    chunk_args = [
        (input_texts, target_texts, max_input_len, max_target_len, i, temp_parquet_dir)
        for i, (input_texts, target_texts) in enumerate(chunks)
    ]
    
    # åˆ›å»ºè¿›ç¨‹æ± å¹¶å¹¶è¡Œå¤„ç†
    tokenize_start = time.time()
    parquet_files = []
    total_samples = 0
    
    with Pool(
        processes=num_proc,
        initializer=init_worker,
        initargs=(model_name, layer_vocab_sizes)
    ) as pool:
        # ä½¿ç”¨imap_unorderedè·å¾—æ›´å¥½çš„æ€§èƒ½
        with tqdm(total=len(chunks), desc="ğŸ”¥ Tokenizing", unit="chunk") as pbar:
            for parquet_file, num_samples in pool.imap_unordered(tokenize_chunk_worker, chunk_args, chunksize=1):
                parquet_files.append(parquet_file)
                total_samples += num_samples
                pbar.update(1)
                pbar.set_postfix({
                    "samples": f"{total_samples:,}",
                    "speed": f"{total_samples/(time.time()-tokenize_start):.0f} samples/s"
                })
    
    tokenize_time = time.time() - tokenize_start
    speed = total_samples / tokenize_time
    
    logger.info(f"\nâœ… Tokenizationå®Œæˆï¼")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
    logger.info(f"   è€—æ—¶: {tokenize_time:.1f}ç§’")
    logger.info(f"   é€Ÿåº¦: {speed:.0f} samples/s")
    logger.info(f"   ç”Ÿæˆæ–‡ä»¶: {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
    
    # æ­¥éª¤3: åˆ†æ‰¹åˆå¹¶parquetæ–‡ä»¶ï¼ˆé¿å…OOMï¼‰
    logger.info("\nğŸ’¾ æ­¥éª¤3: åˆ†æ‰¹åˆå¹¶parquetæ–‡ä»¶...")
    merge_start = time.time()
    
    # åˆ›å»ºä¸´æ—¶Arrowç›®å½•
    temp_arrow_dir = output_path + "_temp_arrow"
    safe_remove_dir(temp_arrow_dir)
    os.makedirs(temp_arrow_dir, exist_ok=True)
    
    # æ’åºparquetæ–‡ä»¶
    parquet_files.sort()
    
    # åˆ†æ‰¹åˆå¹¶ï¼šæ¯æ¬¡50ä¸ªæ–‡ä»¶ï¼ˆçº¦100ä¸‡æ ·æœ¬ï¼Œ~2-3GBå†…å­˜ï¼‰
    merge_batch_size = 50
    num_merge_batches = (len(parquet_files) + merge_batch_size - 1) // merge_batch_size
    
    logger.info(f"åˆ† {num_merge_batches} æ‰¹åˆå¹¶ï¼Œæ¯æ‰¹ {merge_batch_size} ä¸ªæ–‡ä»¶")
    
    all_shards = []
    for i in range(num_merge_batches):
        start_idx = i * merge_batch_size
        end_idx = min((i + 1) * merge_batch_size, len(parquet_files))
        batch_files = parquet_files[start_idx:end_idx]
        
        # è¯»å–å¹¶åˆå¹¶å½“å‰æ‰¹æ¬¡
        tables = [pq.read_table(pf, memory_map=True) for pf in batch_files]
        combined_table = pa.concat_tables(tables)
        del tables
        gc.collect()
        
        # ä¿å­˜ä¸ºArrow shardï¼ˆä½¿ç”¨RecordBatchStreamWriterï¼Œå…¼å®¹HFDataset.from_fileï¼‰
        shard_path = os.path.join(temp_arrow_dir, f"data-{i:05d}-of-{num_merge_batches:05d}.arrow")
        with pa.OSFile(shard_path, 'wb') as sink:
            with pa.ipc.RecordBatchStreamWriter(sink, combined_table.schema) as writer:
                writer.write_table(combined_table)
        
        all_shards.append(shard_path)
        del combined_table
        gc.collect()
    
    merge_time = time.time() - merge_start
    logger.info(f"âœ… åˆå¹¶å®Œæˆï¼Œè€—æ—¶: {merge_time:.1f}ç§’")
    
    # æ­¥éª¤4: åŠ è½½ä¸ºHuggingFace Dataset
    logger.info("\nğŸ“¦ æ­¥éª¤4: åŠ è½½ä¸ºHuggingFace Dataset...")
    load_start = time.time()
    
    # ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½ï¼ˆä¸ä¼šOOMï¼‰
    datasets_list = [HFDataset.from_file(shard) for shard in all_shards]
    dataset = concatenate_datasets(datasets_list)
    del datasets_list
    gc.collect()
    
    logger.info(f"âœ… æ•°æ®é›†å¤§å°: {len(dataset):,} æ ·æœ¬")
    
    # ä¿å­˜æœ€ç»ˆæ•°æ®é›†
    logger.info("\nğŸ’¿ æ­¥éª¤5: ä¿å­˜æœ€ç»ˆæ•°æ®é›†...")
    save_start = time.time()
    
    # æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•
    safe_remove_dir(output_path)
    
    # ç›´æ¥ä¿å­˜åˆ°æœ€ç»ˆä½ç½®
    dataset.save_to_disk(output_path)
    
    del dataset
    gc.collect()
    
    save_time = time.time() - save_start
    logger.info(f"âœ… ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_time:.1f}ç§’")
    
    # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
    logger.info("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    try:
        # åˆ é™¤parquetä¸´æ—¶æ–‡ä»¶
        safe_remove_dir(temp_parquet_dir)
        
        # åˆ é™¤arrowä¸´æ—¶æ–‡ä»¶
        safe_remove_dir(temp_arrow_dir)
        
        logger.info("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {e}")
        logger.warning("ä¸´æ—¶æ–‡ä»¶æœªå®Œå…¨æ¸…ç†ï¼Œä½†ä¸å½±å“æœ€ç»ˆç»“æœ")
    
    # æ€»ç»“
    total_time = time.time() - start_time
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    logger.info(f"å¹³å‡é€Ÿåº¦: {total_samples/total_time:.0f} samples/s")
    logger.info(f"æ•°æ®ä¿å­˜ä½ç½®: {output_path}")
    logger.info("=" * 80)


def preprocess_and_save(config: Config):
    """
    ä¸»å‡½æ•°: é¢„å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹é¢„å¤„ç†tokenizationï¼ˆæé€Ÿå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰")
    logger.info("=" * 80)
    
    model_config = config.generator_t5
    rq_config = config.h_rqkmeans
    
    # å‡†å¤‡å‚æ•°
    layer_vocab_sizes = {
        'l1': rq_config.need_clusters[0],
        'l2': rq_config.need_clusters[1],
        'l3': rq_config.need_clusters[2],
    }
    
    # å®šä¹‰è¾“å…¥è¾“å‡ºè·¯å¾„
    train_tsv = os.path.join(config.output_dir, "generator", "train.tsv")
    val_tsv = os.path.join(config.output_dir, "generator", "val.tsv")
    
    train_output = os.path.join(config.output_dir, "generator", "train_tokenized")
    val_output = os.path.join(config.output_dir, "generator", "val_tokenized")
    
    # å¤„ç†è®­ç»ƒé›†
    if os.path.exists(train_tsv):
        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†è®­ç»ƒé›†")
        logger.info("=" * 80)
        
        if os.path.exists(train_output):
            logger.info(f"æ£€æµ‹åˆ°æ—§çš„é¢„å¤„ç†æ•°æ®ï¼Œæ­£åœ¨åˆ é™¤: {train_output}")
            try:
                safe_remove_dir(train_output)
            except Exception as e:
                logger.error(f"âŒ æ— æ³•åˆ é™¤æ—§æ•°æ®: {e}")
                logger.info("å°è¯•ä½¿ç”¨æ–°çš„ç›®å½•å...")
                train_output = train_output + f"_new_{int(time.time())}"
                logger.info(f"æ–°çš„è¾“å‡ºç›®å½•: {train_output}")
        
        tokenize_dataset_multiproc(
            train_tsv,
            model_config.model_name,
            layer_vocab_sizes,
            model_config.max_input_length,
            model_config.max_target_length,
            train_output,
            chunk_size=20000,  # 20000æ ·æœ¬/chunkï¼ˆå¹³è¡¡å†…å­˜å’Œé€Ÿåº¦ï¼‰
            num_proc=18  # 20æ ¸CPUï¼šä½¿ç”¨18ä¸ªè¿›ç¨‹ï¼ˆä¿ç•™2æ ¸ç»™ç³»ç»Ÿï¼‰
        )
        
        # éªŒè¯æ•°æ®é›†
        logger.info("\néªŒè¯è®­ç»ƒé›†...")
        train_dataset = HFDataset.load_from_disk(train_output)
        logger.info(f"âœ… è®­ç»ƒé›†å¤§å°: {len(train_dataset):,} æ ·æœ¬")
        logger.info(f"âœ… æ•°æ®é›†ç‰¹å¾: {train_dataset.features}")
        
        # æ˜¾ç¤ºæ ·æœ¬
        logger.info("\næ ·æœ¬ç¤ºä¾‹:")
        logger.info(f"  input_idsé•¿åº¦: {len(train_dataset[0]['input_ids'])}")
        logger.info(f"  labelsé•¿åº¦: {len(train_dataset[0]['labels'])}")
        
        del train_dataset
        gc.collect()
        
        # è®¡ç®—ç£ç›˜å ç”¨
        import subprocess
        try:
            size = subprocess.check_output(['du', '-sh', train_output]).split()[0].decode('utf-8')
            logger.info(f"ğŸ’¾ ç£ç›˜å ç”¨: {size}")
        except:
            pass
    else:
        logger.warning(f"âŒ è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨: {train_tsv}")
    
    # å¤„ç†éªŒè¯é›†
    if os.path.exists(val_tsv):
        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†éªŒè¯é›†")
        logger.info("=" * 80)
        
        if os.path.exists(val_output):
            logger.info(f"æ£€æµ‹åˆ°æ—§çš„é¢„å¤„ç†æ•°æ®ï¼Œæ­£åœ¨åˆ é™¤: {val_output}")
            try:
                safe_remove_dir(val_output)
            except Exception as e:
                logger.error(f"âŒ æ— æ³•åˆ é™¤æ—§æ•°æ®: {e}")
                logger.info("å°è¯•ä½¿ç”¨æ–°çš„ç›®å½•å...")
                val_output = val_output + f"_new_{int(time.time())}"
                logger.info(f"æ–°çš„è¾“å‡ºç›®å½•: {val_output}")
        
        tokenize_dataset_multiproc(
            val_tsv,
            model_config.model_name,
            layer_vocab_sizes,
            model_config.max_input_length,
            model_config.max_target_length,
            val_output,
            chunk_size=20000,
            num_proc=18
        )
        
        logger.info("\néªŒè¯éªŒè¯é›†...")
        val_dataset = HFDataset.load_from_disk(val_output)
        logger.info(f"âœ… éªŒè¯é›†å¤§å°: {len(val_dataset):,} æ ·æœ¬")
        logger.info(f"âœ… æ•°æ®é›†ç‰¹å¾: {val_dataset.features}")
        del val_dataset
        gc.collect()
        
        try:
            size = subprocess.check_output(['du', '-sh', val_output]).split()[0].decode('utf-8')
            logger.info(f"ğŸ’¾ ç£ç›˜å ç”¨: {size}")
        except:
            pass
    else:
        logger.warning(f"âŒ éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {val_tsv}")
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ é¢„å¤„ç†å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"âœ… è®­ç»ƒé›†ä¿å­˜ä½ç½®: {train_output}")
    logger.info(f"âœ… éªŒè¯é›†ä¿å­˜ä½ç½®: {val_output}")
    logger.info("\nç°åœ¨å¯ä»¥ä½¿ç”¨ä¼˜åŒ–åçš„è®­ç»ƒè„šæœ¬è¿›è¡Œè®­ç»ƒ")


if __name__ == "__main__":
    config = Config()
    log_file_path = os.path.join(config.log_dir, "preprocess_tokenize_fast.log")
    setup_logging(log_file=log_file_path)
    logger = logging.getLogger(__name__)
    
    preprocess_and_save(config)

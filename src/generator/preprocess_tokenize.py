"""
é¢„å¤„ç†è„šæœ¬: å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ - ä¿®å¤OOMé—®é¢˜
ä¸»è¦æ”¹è¿›:
1. å‡å°chunk_size (20000 -> 10000) é™ä½å†…å­˜å³°å€¼
2. æµå¼åˆå¹¶parquetæ–‡ä»¶ï¼Œè¾¹å¤„ç†è¾¹åˆå¹¶ï¼Œé¿å…æ–‡ä»¶å †ç§¯
3. ä¿®å¤è¾“å‡ºé•¿åº¦ç»Ÿè®¡ï¼šæŒ‰è¯­ä¹‰IDç²’åº¦ç»Ÿè®¡
4. å¢å¼ºå†…å­˜æ¸…ç†å’Œåƒåœ¾å›æ”¶
"""
import os
import sys
import logging
import shutil
from tqdm import tqdm
from datasets import Dataset as HFDataset, concatenate_datasets
import gc
import pyarrow as pa
import pyarrow.parquet as pq
from typing import List, Tuple
from multiprocessing import Pool, cpu_count
import time
import re

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from config_optimized import Config
except ImportError:
    from config import Config
from src.generator.tiger_model import TIGERTokenizer
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šworkerè¿›ç¨‹çš„tokenizer
worker_tokenizer = None


def safe_remove_dir(path: str, max_retries: int = 3, retry_delay: float = 1.0):
    """
    å®‰å…¨åˆ é™¤ç›®å½•ï¼Œå¤„ç†å¯èƒ½çš„æ–‡ä»¶é”å®šé—®é¢˜
    å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰
    
    Args:
        path: è¦åˆ é™¤çš„ç›®å½•è·¯å¾„
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    """
    if not os.path.exists(path):
        logger.debug(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {path}")
        return
    
    for attempt in range(max_retries):
        try:
            shutil.rmtree(path)
            logger.info(f"âœ… å·²åˆ é™¤ç›®å½•: {path}")
            return
        except OSError as e:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ åˆ é™¤ç›®å½•å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å¯èƒ½çš„æ–‡ä»¶å¥æŸ„
            else:
                logger.error(f"âŒ æ— æ³•åˆ é™¤ç›®å½• {path} (å·²é‡è¯• {max_retries} æ¬¡): {e}")
                logger.warning(f"âš ï¸ è¯·æ‰‹åŠ¨åˆ é™¤è¯¥ç›®å½•: {path}")
                raise


def init_worker(model_name: str, layer_vocab_sizes: dict):
    """
    åˆå§‹åŒ–workerè¿›ç¨‹çš„tokenizerï¼ˆæ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªç‹¬ç«‹çš„tokenizerï¼‰
    """
    global worker_tokenizer
    from src.generator.tiger_model_new import TIGERTokenizer
    worker_tokenizer = TIGERTokenizer(base_model=model_name, layer_vocab_sizes=layer_vocab_sizes)


def count_semantic_ids(text: str) -> int:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­çš„è¯­ä¹‰IDæ•°é‡
    
    Args:
        text: åŒ…å«è¯­ä¹‰IDçš„æ–‡æœ¬ï¼Œå¦‚ "<id_l1_3> <id_l2_99> ..."
    
    Returns:
        è¯­ä¹‰IDçš„æ•°é‡
    """
    # åŒ¹é… <id_l1_xxx> <id_l2_xxx> <id_l3_xxx> æ ¼å¼
    pattern = r'<id_l[123]_\d+>'
    matches = re.findall(pattern, text)
    return len(matches)


def tokenize_chunk_worker(args: Tuple[List[str], List[str], int, int, int, str]) -> Tuple[str, int]:
    """
    Workerå‡½æ•°ï¼štokenizeä¸€ä¸ªæ•°æ®å—
    
    Args:
        args: (input_texts, target_texts, max_input_len, max_target_len, chunk_id, temp_dir)
    
    Returns:
        (parquet_file_path, num_samples)
    """
    input_texts, target_texts, max_input_len, max_target_len, chunk_id, temp_dir = args
    
    # ä½¿ç”¨workerçš„tokenizer
    global worker_tokenizer
    
    # æ‰¹é‡tokenizeï¼ˆHuggingFace tokenizerå†…éƒ¨å·²ä¼˜åŒ–ï¼‰
    input_encodings = worker_tokenizer.base_tokenizer(
        input_texts,
        max_length=max_input_len,
        truncation=True,
        padding='max_length',
        return_tensors=None
    )
    
    target_encodings = worker_tokenizer.base_tokenizer(
        target_texts,
        max_length=max_target_len,
        truncation=True,
        padding='max_length',
        return_tensors=None
    )
    
    # é‡‡æ ·æ‰“å°ï¼šåªåœ¨ç¬¬ä¸€ä¸ªchunkï¼ˆchunk_id=0ï¼‰æ‰“å°5æ¡æ ·æœ¬
    if chunk_id == 0:
        print("\n" + "=" * 100)
        print(f"ğŸ“‹ é‡‡æ ·æ£€æŸ¥ - Chunk {chunk_id} çš„å‰5æ¡æ•°æ®")
        print("=" * 100)
        
        num_samples_to_print = min(5, len(input_texts))
        for i in range(num_samples_to_print):
            print(f"\n{'â”€' * 100}")
            print(f"æ ·æœ¬ #{i+1}")
            print(f"{'â”€' * 100}")
            
            # åŸå§‹è¾“å…¥
            print(f"\nã€åŸå§‹è¾“å…¥ã€‘")
            print(f"  æ–‡æœ¬: {input_texts[i][:200]}{'...' if len(input_texts[i]) > 200 else ''}")
            print(f"  é•¿åº¦: {len(input_texts[i])} å­—ç¬¦")
            
            # åŸå§‹è¾“å‡º - ä¿®å¤ï¼šæŒ‰è¯­ä¹‰IDç²’åº¦ç»Ÿè®¡
            num_semantic_ids = count_semantic_ids(target_texts[i])
            print(f"\nã€åŸå§‹è¾“å‡ºã€‘")
            print(f"  æ–‡æœ¬: {target_texts[i][:200]}{'...' if len(target_texts[i]) > 200 else ''}")
            print(f"  å­—ç¬¦é•¿åº¦: {len(target_texts[i])} å­—ç¬¦")
            print(f"  è¯­ä¹‰IDæ•°é‡: {num_semantic_ids} ä¸ª")
            
            # Tokenizeåçš„è¾“å…¥
            input_ids = input_encodings['input_ids'][i]
            attention_mask = input_encodings['attention_mask'][i]
            print(f"\nã€Tokenizeåçš„è¾“å…¥ã€‘")
            print(f"  input_ids: {input_ids[:50]}{'...' if len(input_ids) > 50 else ''}")
            print(f"  input_idsé•¿åº¦: {len(input_ids)}")
            print(f"  æœ‰æ•ˆtokenæ•°: {sum(attention_mask)}")
            print(f"  paddingæ•°: {len(attention_mask) - sum(attention_mask)}")
            
            # Tokenizeåçš„è¾“å‡º
            label_ids = target_encodings['input_ids'][i]
            label_attention = target_encodings['attention_mask'][i]
            print(f"\nã€Tokenizeåçš„è¾“å‡ºã€‘")
            print(f"  label_ids: {label_ids[:50]}{'...' if len(label_ids) > 50 else ''}")
            print(f"  label_idsé•¿åº¦: {len(label_ids)}")
            print(f"  æœ‰æ•ˆtokenæ•°: {sum(label_attention)}")
            print(f"  paddingæ•°: {len(label_attention) - sum(label_attention)}")
            
            # è§£ç éªŒè¯ï¼ˆå‰50ä¸ªtokenï¼‰
            decoded_input = worker_tokenizer.base_tokenizer.decode(
                [tid for tid in input_ids[:50] if tid != worker_tokenizer.base_tokenizer.pad_token_id],
                skip_special_tokens=False
            )
            decoded_target = worker_tokenizer.base_tokenizer.decode(
                [tid for tid in label_ids[:50] if tid != worker_tokenizer.base_tokenizer.pad_token_id],
                skip_special_tokens=False
            )
            print(f"\nã€è§£ç éªŒè¯ï¼ˆå‰50ä¸ªtokenï¼‰ã€‘")
            print(f"  è¾“å…¥è§£ç : {decoded_input}")
            print(f"  è¾“å‡ºè§£ç : {decoded_target}")
        
        print(f"\n{'=' * 100}")
        print(f"âœ… é‡‡æ ·æ£€æŸ¥å®Œæˆ")
        print(f"{'=' * 100}\n")
    
    # æ„å»ºArrow Tableï¼ˆé›¶æ‹·è´ï¼‰
    schema = pa.schema([
        ('input_ids', pa.list_(pa.int64())),
        ('attention_mask', pa.list_(pa.int64())),
        ('labels', pa.list_(pa.int64()))
    ])
    
    arrays = [
        pa.array(input_encodings['input_ids']),
        pa.array(input_encodings['attention_mask']),
        pa.array(target_encodings['input_ids'])
    ]
    table = pa.Table.from_arrays(arrays, schema=schema)
    
    # å†™å…¥parquetæ–‡ä»¶ï¼ˆsnappyå‹ç¼©ï¼‰
    parquet_file = os.path.join(temp_dir, f"chunk_{chunk_id:06d}.parquet")
    pq.write_table(table, parquet_file, compression='snappy')
    
    # ç«‹å³é‡Šæ”¾å†…å­˜
    del input_encodings, target_encodings, arrays, table
    gc.collect()
    
    return parquet_file, len(input_texts)


def read_and_split_data(data_path: str, chunk_size: int) -> List[Tuple[List[str], List[str]]]:
    """
    å¿«é€Ÿè¯»å–TSVæ–‡ä»¶å¹¶åˆ†å‰²æˆå¤šä¸ªchunkï¼ˆç”¨äºå¤šè¿›ç¨‹å¤„ç†ï¼‰
    
    Args:
        data_path: TSVæ–‡ä»¶è·¯å¾„
        chunk_size: æ¯ä¸ªchunkçš„å¤§å°
    
    Returns:
        List of (input_texts, target_texts) tuples
    """
    logger.info(f"è¯»å–æ–‡ä»¶: {data_path}")
    logger.info(f"Chunkå¤§å°: {chunk_size:,} æ ·æœ¬/chunk")
    
    chunks = []
    input_texts = []
    target_texts = []
    total_lines = 0
    
    # ä½¿ç”¨å¤§ç¼“å†²åŒºåŠ é€Ÿè¯»å–
    with open(data_path, 'r', encoding='utf-8', buffering=32*1024*1024) as f:  # 32MBç¼“å†²
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) != 3:
                continue
            
            glid, input_text, target_text = parts
            input_texts.append(input_text)
            target_texts.append(target_text)
            total_lines += 1
            
            if len(input_texts) >= chunk_size:
                chunks.append((input_texts, target_texts))
                input_texts = []
                target_texts = []
    
    # æ·»åŠ å‰©ä½™æ•°æ®
    if input_texts:
        chunks.append((input_texts, target_texts))
    
    logger.info(f"æ€»æ ·æœ¬æ•°: {total_lines:,}")
    logger.info(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªchunks")
    
    return chunks


def merge_parquet_files_streaming(parquet_files: List[str], output_dir: str, batch_size: int = 30) -> List[str]:
    """
    æµå¼åˆå¹¶parquetæ–‡ä»¶ï¼Œé¿å…å†…å­˜å †ç§¯
    
    Args:
        parquet_files: parquetæ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        batch_size: æ¯æ‰¹åˆå¹¶çš„æ–‡ä»¶æ•°ï¼ˆé™ä½åˆ°30ä»¥å‡å°‘å†…å­˜ï¼‰
    
    Returns:
        åˆå¹¶åçš„Arrowæ–‡ä»¶åˆ—è¡¨
    """
    logger.info(f"\nğŸ’¾ æµå¼åˆå¹¶parquetæ–‡ä»¶...")
    logger.info(f"  æ€»æ–‡ä»¶æ•°: {len(parquet_files)}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size} æ–‡ä»¶/æ‰¹")
    
    # æ’åºæ–‡ä»¶
    parquet_files.sort()
    
    # åˆ†æ‰¹åˆå¹¶
    num_batches = (len(parquet_files) + batch_size - 1) // batch_size
    logger.info(f"  åˆ† {num_batches} æ‰¹åˆå¹¶")
    
    arrow_shards = []
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, len(parquet_files))
        batch_files = parquet_files[start_idx:end_idx]
        
        logger.info(f"  åˆå¹¶æ‰¹æ¬¡ {i+1}/{num_batches} ({len(batch_files)} æ–‡ä»¶)...")
        
        # è¯»å–å¹¶åˆå¹¶å½“å‰æ‰¹æ¬¡
        tables = []
        for pf in batch_files:
            tables.append(pq.read_table(pf, memory_map=True))
        
        combined_table = pa.concat_tables(tables)
        del tables
        gc.collect()
        
        # ä¿å­˜ä¸ºArrow shard
        shard_path = os.path.join(output_dir, f"data-{i:05d}-of-{num_batches:05d}.arrow")
        with pa.OSFile(shard_path, 'wb') as sink:
            with pa.ipc.RecordBatchStreamWriter(sink, combined_table.schema) as writer:
                writer.write_table(combined_table)
        
        arrow_shards.append(shard_path)
        
        # ç«‹å³åˆ é™¤å·²åˆå¹¶çš„parquetæ–‡ä»¶ï¼Œé‡Šæ”¾ç£ç›˜ç©ºé—´
        for pf in batch_files:
            try:
                os.remove(pf)
            except:
                pass
        
        del combined_table
        gc.collect()
        
        logger.info(f"  âœ… æ‰¹æ¬¡ {i+1}/{num_batches} å®Œæˆ")
    
    logger.info(f"âœ… æµå¼åˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(arrow_shards)} ä¸ªArrowæ–‡ä»¶")
    return arrow_shards


def tokenize_dataset_multiproc(
    data_path: str,
    model_name: str,
    layer_vocab_sizes: dict,
    max_input_len: int,
    max_target_len: int,
    output_path: str,
    chunk_size: int = 10000,  # é™ä½åˆ°10000ä»¥å‡å°‘å†…å­˜å³°å€¼
    num_proc: int = None
):
    """
    å¤šè¿›ç¨‹å¹¶è¡Œtokenize - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. å‡å°chunk_size (20000 -> 10000) é™ä½å†…å­˜å³°å€¼
    2. æµå¼åˆå¹¶parquetæ–‡ä»¶ï¼Œè¾¹å¤„ç†è¾¹åˆå¹¶
    3. åŠæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    4. å¢å¼ºåƒåœ¾å›æ”¶
    
    Args:
        data_path: TSVæ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        layer_vocab_sizes: å±‚çº§è¯è¡¨å¤§å°
        max_input_len: è¾“å…¥æœ€å¤§é•¿åº¦
        max_target_len: ç›®æ ‡æœ€å¤§é•¿åº¦
        output_path: è¾“å‡ºè·¯å¾„
        chunk_size: æ¯ä¸ªchunkçš„æ ·æœ¬æ•°ï¼ˆé™ä½åˆ°10000ï¼‰
        num_proc: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤CPUæ ¸å¿ƒæ•°-2ï¼‰
    """
    if num_proc is None:
        num_proc = max(1, cpu_count() - 2)
    
    logger.info("=" * 80)
    logger.info("ğŸš€ å¤šè¿›ç¨‹å¹¶è¡ŒTokenizationï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    logger.info("=" * 80)
    logger.info(f"æ–‡ä»¶è·¯å¾„: {data_path}")
    logger.info(f"Chunkå¤§å°: {chunk_size:,} æ ·æœ¬/chunk (é™ä½ä»¥å‡å°‘å†…å­˜)")
    logger.info(f"å¹¶è¡Œè¿›ç¨‹æ•°: {num_proc} (CPUæ ¸å¿ƒæ•°: {cpu_count()})")
    logger.info(f"å†…å­˜ä¼˜åŒ–: æµå¼åˆå¹¶ + åŠæ—¶æ¸…ç†")
    
    start_time = time.time()
    
    # å‡†å¤‡ä¸´æ—¶ç›®å½•
    temp_parquet_dir = output_path + "_temp_parquet"
    safe_remove_dir(temp_parquet_dir)
    os.makedirs(temp_parquet_dir, exist_ok=True)
    
    # æ­¥éª¤1: å¿«é€Ÿè¯»å–å¹¶åˆ†å‰²æ•°æ®
    logger.info("\nğŸ“– æ­¥éª¤1: è¯»å–å¹¶åˆ†å‰²æ•°æ®...")
    read_start = time.time()
    chunks = read_and_split_data(data_path, chunk_size)
    read_time = time.time() - read_start
    logger.info(f"âœ… è¯»å–å®Œæˆï¼Œè€—æ—¶: {read_time:.1f}ç§’")
    
    # æ­¥éª¤2: å¤šè¿›ç¨‹å¹¶è¡Œtokenization
    logger.info(f"\nâš¡ æ­¥éª¤2: å¯åŠ¨ {num_proc} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œtokenization...")
    
    # å‡†å¤‡å‚æ•°
    chunk_args = [
        (input_texts, target_texts, max_input_len, max_target_len, i, temp_parquet_dir)
        for i, (input_texts, target_texts) in enumerate(chunks)
    ]
    
    # åˆ›å»ºè¿›ç¨‹æ± å¹¶å¹¶è¡Œå¤„ç†
    tokenize_start = time.time()
    parquet_files = []
    total_samples = 0
    
    with Pool(
        processes=num_proc,
        initializer=init_worker,
        initargs=(model_name, layer_vocab_sizes)
    ) as pool:
        # ä½¿ç”¨imap_unorderedè·å¾—æ›´å¥½çš„æ€§èƒ½
        with tqdm(total=len(chunks), desc="ğŸ”¥ Tokenizing", unit="chunk") as pbar:
            for parquet_file, num_samples in pool.imap_unordered(tokenize_chunk_worker, chunk_args, chunksize=1):
                parquet_files.append(parquet_file)
                total_samples += num_samples
                pbar.update(1)
                pbar.set_postfix({
                    "samples": f"{total_samples:,}",
                    "speed": f"{total_samples/(time.time()-tokenize_start):.0f} samples/s"
                })
    
    tokenize_time = time.time() - tokenize_start
    speed = total_samples / tokenize_time
    
    logger.info(f"\nâœ… Tokenizationå®Œæˆï¼")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
    logger.info(f"   è€—æ—¶: {tokenize_time:.1f}ç§’")
    logger.info(f"   é€Ÿåº¦: {speed:.0f} samples/s")
    logger.info(f"   ç”Ÿæˆæ–‡ä»¶: {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
    
    # æ­¥éª¤3: æµå¼åˆå¹¶parquetæ–‡ä»¶
    temp_arrow_dir = output_path + "_temp_arrow"
    safe_remove_dir(temp_arrow_dir)
    os.makedirs(temp_arrow_dir, exist_ok=True)
    
    merge_start = time.time()
    arrow_shards = merge_parquet_files_streaming(
        parquet_files, 
        temp_arrow_dir, 
        batch_size=30  # é™ä½æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å†…å­˜
    )
    merge_time = time.time() - merge_start
    logger.info(f"âœ… åˆå¹¶å®Œæˆï¼Œè€—æ—¶: {merge_time:.1f}ç§’")
    
    # æ­¥éª¤4: åŠ è½½ä¸ºHuggingFace Dataset
    logger.info("\nğŸ“¦ æ­¥éª¤4: åŠ è½½ä¸ºHuggingFace Dataset...")
    load_start = time.time()
    
    # ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½
    datasets_list = []
    for shard in arrow_shards:
        datasets_list.append(HFDataset.from_file(shard))
    
    dataset = concatenate_datasets(datasets_list)
    del datasets_list
    gc.collect()
    
    logger.info(f"âœ… æ•°æ®é›†å¤§å°: {len(dataset):,} æ ·æœ¬")
    
    # ä¿å­˜æœ€ç»ˆæ•°æ®é›†
    logger.info("\nğŸ’¿ æ­¥éª¤5: ä¿å­˜æœ€ç»ˆæ•°æ®é›†...")
    save_start = time.time()
    
    # æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•
    safe_remove_dir(output_path)
    
    # ç›´æ¥ä¿å­˜åˆ°æœ€ç»ˆä½ç½®
    dataset.save_to_disk(output_path)
    
    del dataset
    gc.collect()
    
    save_time = time.time() - save_start
    logger.info(f"âœ… ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_time:.1f}ç§’")
    
    # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
    logger.info("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    try:
        safe_remove_dir(temp_parquet_dir)
        safe_remove_dir(temp_arrow_dir)
        logger.info("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    # æ€»ç»“
    total_time = time.time() - start_time
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    logger.info(f"å¹³å‡é€Ÿåº¦: {total_samples/total_time:.0f} samples/s")
    logger.info(f"æ•°æ®ä¿å­˜ä½ç½®: {output_path}")
    logger.info("=" * 80)


def preprocess_and_save(config: Config):
    """
    ä¸»å‡½æ•°: é¢„å¤„ç†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹é¢„å¤„ç†tokenizationï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    logger.info("=" * 80)
    
    model_config = config.generator_t5
    rq_config = config.h_rqkmeans
    
    # å‡†å¤‡å‚æ•°
    layer_vocab_sizes = {
        'l1': rq_config.need_clusters[0],
        'l2': rq_config.need_clusters[1],
        'l3': rq_config.need_clusters[2],
    }
    
    logger.info(f"\nğŸ“Š Tokenizeré…ç½®:")
    logger.info(f"  æ¨¡å‹: {model_config.model_name}")
    logger.info(f"  Layer 1 è¯è¡¨å¤§å°: {layer_vocab_sizes['l1']}")
    logger.info(f"  Layer 2 è¯è¡¨å¤§å°: {layer_vocab_sizes['l2']}")
    logger.info(f"  Layer 3 è¯è¡¨å¤§å°: {layer_vocab_sizes['l3']}")
    logger.info(f"  æ€»è¯­ä¹‰ID tokens: {sum(layer_vocab_sizes.values())}")
    
    # å®šä¹‰è¾“å…¥è¾“å‡ºè·¯å¾„
    train_tsv = os.path.join(config.output_dir, "generator", "train.tsv")
    val_tsv = os.path.join(config.output_dir, "generator", "val.tsv")
    test_tsv = os.path.join(config.output_dir, "generator", "test.tsv")
    
    train_output = os.path.join(config.output_dir, "generator", "train_tokenized")
    val_output = os.path.join(config.output_dir, "generator", "val_tokenized")
    test_output = os.path.join(config.output_dir, "generator", "test_tokenized")
    
    # å¤„ç†è®­ç»ƒé›†
    if os.path.exists(train_tsv):
        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†è®­ç»ƒé›†")
        logger.info("=" * 80)
        
        if os.path.exists(train_output):
            logger.info(f"æ£€æµ‹åˆ°æ—§çš„é¢„å¤„ç†æ•°æ®ï¼Œæ­£åœ¨åˆ é™¤: {train_output}")
            try:
                safe_remove_dir(train_output)
            except Exception as e:
                logger.error(f"âŒ æ— æ³•åˆ é™¤æ—§æ•°æ®: {e}")
                logger.info("å°è¯•ä½¿ç”¨æ–°çš„ç›®å½•å...")
                train_output = train_output + f"_new_{int(time.time())}"
                logger.info(f"æ–°çš„è¾“å‡ºç›®å½•: {train_output}")
        
        tokenize_dataset_multiproc(
            train_tsv,
            model_config.model_name,
            layer_vocab_sizes,
            model_config.max_input_length,
            model_config.max_target_length,
            train_output,
            chunk_size=10000,  # é™ä½chunk_sizeä»¥å‡å°‘å†…å­˜
            num_proc=16  # é™ä½è¿›ç¨‹æ•°ï¼ˆä»18é™åˆ°16ï¼‰ä»¥å‡å°‘å†…å­˜å‹åŠ›
        )
        
        # éªŒè¯æ•°æ®é›†
        logger.info("\néªŒè¯è®­ç»ƒé›†...")
        train_dataset = HFDataset.load_from_disk(train_output)
        logger.info(f"âœ… è®­ç»ƒé›†å¤§å°: {len(train_dataset):,} æ ·æœ¬")
        logger.info(f"âœ… æ•°æ®é›†ç‰¹å¾: {train_dataset.features}")
        
        # æ˜¾ç¤ºæ ·æœ¬
        logger.info("\næ ·æœ¬ç¤ºä¾‹:")
        logger.info(f"  input_idsé•¿åº¦: {len(train_dataset[0]['input_ids'])}")
        logger.info(f"  labelsé•¿åº¦: {len(train_dataset[0]['labels'])}")
        
        del train_dataset
        gc.collect()
    else:
        logger.warning(f"âŒ è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨: {train_tsv}")
    
    # å¤„ç†éªŒè¯é›†
    if os.path.exists(val_tsv):
        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†éªŒè¯é›†")
        logger.info("=" * 80)
        
        if os.path.exists(val_output):
            logger.info(f"æ£€æµ‹åˆ°æ—§çš„é¢„å¤„ç†æ•°æ®ï¼Œæ­£åœ¨åˆ é™¤: {val_output}")
            try:
                safe_remove_dir(val_output)
            except Exception as e:
                logger.error(f"âŒ æ— æ³•åˆ é™¤æ—§æ•°æ®: {e}")
                logger.info("å°è¯•ä½¿ç”¨æ–°çš„ç›®å½•å...")
                val_output = val_output + f"_new_{int(time.time())}"
                logger.info(f"æ–°çš„è¾“å‡ºç›®å½•: {val_output}")
        
        tokenize_dataset_multiproc(
            val_tsv,
            model_config.model_name,
            layer_vocab_sizes,
            model_config.max_input_length,
            model_config.max_target_length,
            val_output,
            chunk_size=10000,
            num_proc=16
        )
        
        logger.info("\néªŒè¯éªŒè¯é›†...")
        val_dataset = HFDataset.load_from_disk(val_output)
        logger.info(f"âœ… éªŒè¯é›†å¤§å°: {len(val_dataset):,} æ ·æœ¬")
        logger.info(f"âœ… æ•°æ®é›†ç‰¹å¾: {val_dataset.features}")
        del val_dataset
        gc.collect()
    else:
        logger.warning(f"âŒ éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {val_tsv}")
    
    # å¤„ç†æµ‹è¯•é›†
    if os.path.exists(test_tsv):
        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†æµ‹è¯•é›†")
        logger.info("=" * 80)
        
        if os.path.exists(test_output):
            logger.info(f"æ£€æµ‹åˆ°æ—§çš„é¢„å¤„ç†æ•°æ®ï¼Œæ­£åœ¨åˆ é™¤: {test_output}")
            try:
                safe_remove_dir(test_output)
            except Exception as e:
                logger.error(f"âŒ æ— æ³•åˆ é™¤æ—§æ•°æ®: {e}")
                logger.info("å°è¯•ä½¿ç”¨æ–°çš„ç›®å½•å...")
                test_output = test_output + f"_new_{int(time.time())}"
                logger.info(f"æ–°çš„è¾“å‡ºç›®å½•: {test_output}")
        
        tokenize_dataset_multiproc(
            test_tsv,
            model_config.model_name,
            layer_vocab_sizes,
            model_config.max_input_length,
            model_config.max_target_length,
            test_output,
            chunk_size=10000,
            num_proc=16
        )
        
        logger.info("\néªŒè¯æµ‹è¯•é›†...")
        test_dataset = HFDataset.load_from_disk(test_output)
        logger.info(f"âœ… æµ‹è¯•é›†å¤§å°: {len(test_dataset):,} æ ·æœ¬")
        logger.info(f"âœ… æ•°æ®é›†ç‰¹å¾: {test_dataset.features}")
        del test_dataset
        gc.collect()
    else:
        logger.warning(f"âŒ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {test_tsv}")
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ é¢„å¤„ç†å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"âœ… è®­ç»ƒé›†ä¿å­˜ä½ç½®: {train_output}")
    logger.info(f"âœ… éªŒè¯é›†ä¿å­˜ä½ç½®: {val_output}")
    if os.path.exists(test_tsv):
        logger.info(f"âœ… æµ‹è¯•é›†ä¿å­˜ä½ç½®: {test_output}")
    logger.info("\nç°åœ¨å¯ä»¥ä½¿ç”¨ä¼˜åŒ–åçš„è®­ç»ƒè„šæœ¬è¿›è¡Œè®­ç»ƒ")


if __name__ == "__main__":
    config = Config()
    log_file_path = os.path.join(config.log_dir, "preprocess_tokenize_fixed.log")
    setup_logging(log_file=log_file_path)
    logger = logging.getLogger(__name__)
    
    preprocess_and_save(config)

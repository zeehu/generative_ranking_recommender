"""
Step G3: Train the T5 Generator Model. 
"""
import os
import sys
import logging
import torch
import numpy as np
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import Config
from src.generator.tiger_model import TIGERModel, TIGERTokenizer
from src.common.utils import set_seed, setup_logging

logger = logging.getLogger(__name__)

class PlaylistDataset(Dataset):
    """Loads the corpus generated by prepare_corpus.py with pre-tokenization for efficiency"""
    def __init__(self, data_path: str, tokenizer: TIGERTokenizer, max_input_len: int, max_target_len: int, cache_tokenization: bool = True):
        logger.info(f"Loading data from {data_path}...")
        self.tokenizer = tokenizer
        self.max_input_len = max_input_len
        self.max_target_len = max_target_len
        self.cache_tokenization = cache_tokenization
        
        # Load raw data
        raw_data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) == 3:
                    _glid, input_text, target_text = parts
                    raw_data.append((input_text, target_text))
        
        logger.info(f"Loaded {len(raw_data)} samples.")
        
        # Pre-tokenize if caching is enabled (recommended for training efficiency)
        if cache_tokenization:
            logger.info("Pre-tokenizing dataset for faster training...")
            self.data = []
            from tqdm import tqdm
            for input_text, target_text in tqdm(raw_data, desc="Tokenizing"):
                input_encoding = self.tokenizer.base_tokenizer(
                    input_text, 
                    max_length=self.max_input_len, 
                    truncation=True,
                    padding=False  # Padding will be done by DataCollator
                )
                target_encoding = self.tokenizer.base_tokenizer(
                    target_text, 
                    max_length=self.max_target_len, 
                    truncation=True,
                    padding=False
                )
                self.data.append({
                    "input_ids": input_encoding.input_ids,
                    "attention_mask": input_encoding.attention_mask,
                    "labels": target_encoding.input_ids
                })
            logger.info("Pre-tokenization complete.")
        else:
            self.data = raw_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if self.cache_tokenization:
            # Return pre-tokenized data
            return self.data[idx]
        else:
            # Tokenize on-the-fly (slower but uses less memory)
            input_text, target_text = self.data[idx]
            input_encoding = self.tokenizer.base_tokenizer(
                input_text, 
                max_length=self.max_input_len, 
                truncation=True,
                padding=False
            )
            target_encoding = self.tokenizer.base_tokenizer(
                target_text, 
                max_length=self.max_target_len, 
                truncation=True,
                padding=False
            )
            return {
                "input_ids": input_encoding.input_ids, 
                "attention_mask": input_encoding.attention_mask, 
                "labels": target_encoding.input_ids
            }

class T5Trainer:
    def __init__(self, config: Config):
        self.config = config
        set_seed(config.seed)

    def run(self):
        logger.info("--- Starting Step G3: T5 Generator Model Training ---")
        model_config = self.config.generator_t5
        rq_config = self.config.h_rqkmeans

        # Calculate the layer-specific vocab sizes for semantic ID tokens
        # need_clusters = [128, 128, 128] means:
        #   Layer 1: 128 different IDs (0-127)
        #   Layer 2: 128 different IDs (0-127)
        #   Layer 3: 128 different IDs (0-127)
        layer_vocab_sizes = {
            'l1': rq_config.need_clusters[0],
            'l2': rq_config.need_clusters[1],
            'l3': rq_config.need_clusters[2],
        }
        
        logger.info(f"Layer vocab sizes: {layer_vocab_sizes}")
        logger.info(f"Total semantic ID tokens: {sum(layer_vocab_sizes.values())} + 2 special tokens")
        
        # Initialize TIGERModel and TIGERTokenizer
        # The model will add layer-specific semantic ID tokens to the tokenizer
        model = TIGERModel(base_model=model_config.model_name, layer_vocab_sizes=layer_vocab_sizes)
        model.model.config.use_cache = False  # Necessary for gradient checkpointing
        tokenizer = model.tokenizer
        
        logger.info(f"Tokenizer vocab size: {len(tokenizer)}")

        train_dataset = PlaylistDataset(
            os.path.join(self.config.output_dir, "generator", "train.tsv"), 
            tokenizer, 
            model_config.max_input_length, 
            model_config.max_target_length
        )
        val_dataset = PlaylistDataset(
            os.path.join(self.config.output_dir, "generator", "val.tsv"), 
            tokenizer, 
            model_config.max_input_length, 
            model_config.max_target_length
        )

        training_args = TrainingArguments(
            output_dir=os.path.join(self.config.model_dir, "generator", "checkpoints"),
            num_train_epochs=model_config.num_train_epochs,
            per_device_train_batch_size=model_config.per_device_train_batch_size,
            gradient_accumulation_steps=model_config.gradient_accumulation_steps,
            learning_rate=model_config.learning_rate,
            warmup_steps=model_config.warmup_steps,
            weight_decay=model_config.weight_decay,
            fp16=model_config.fp16,
            gradient_checkpointing=True,
            gradient_checkpointing_kwargs=model_config.gradient_checkpointing_kwargs,
            max_grad_norm=1.0,
            eval_strategy="steps",
            eval_steps=2000,
            save_strategy="steps",
            save_steps=2000,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            logging_steps=100,
            report_to="none",
            dataloader_num_workers=self.config.num_workers,
            remove_unused_columns=False,
        )

        # Check torch.compile compatibility
        use_compile = False
        if torch.__version__ >= "2.0.0":
            # torch.compile may not work well with gradient checkpointing in some versions
            # Disable it if gradient checkpointing is enabled
            if not training_args.gradient_checkpointing:
                logger.info("Enabling torch.compile for model optimization...")
                model.model = torch.compile(model.model)
                use_compile = True
            else:
                logger.info("Skipping torch.compile due to gradient checkpointing (may cause compatibility issues)")
        
        trainer = Trainer(
            model=model.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer.base_tokenizer, model=model.model)
        )

        logger.info("Starting training... This may take a while.")
        logger.info(f"Training configuration:")
        logger.info(f"  - Epochs: {model_config.num_train_epochs}")
        logger.info(f"  - Batch size: {model_config.per_device_train_batch_size}")
        logger.info(f"  - Gradient accumulation: {model_config.gradient_accumulation_steps}")
        logger.info(f"  - Effective batch size: {model_config.per_device_train_batch_size * model_config.gradient_accumulation_steps}")
        logger.info(f"  - Learning rate: {model_config.learning_rate}")
        logger.info(f"  - FP16: {model_config.fp16}")
        logger.info(f"  - Gradient checkpointing: {training_args.gradient_checkpointing}")
        logger.info(f"  - Torch compile: {use_compile}")
        
        trainer.train()

        final_model_path = os.path.join(self.config.model_dir, "generator", "final_model")
        model.save_pretrained(final_model_path)
        logger.info(f"Training complete. Final generator model saved to {final_model_path}")
        logger.info("--- Step G3 Completed Successfully ---")

if __name__ == "__main__":
    config = Config()
    log_file_path = os.path.join(config.log_dir, "g3_train_t5.log")
    setup_logging(log_file=log_file_path)
    logger = logging.getLogger(__name__)
    trainer = T5Trainer(config)
    trainer.run()
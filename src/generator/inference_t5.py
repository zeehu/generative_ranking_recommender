"""
T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„T5æ¨¡å‹å¹¶æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ­Œæ›²æ¨è
"""
import os
import sys
import torch
import json
import logging
import random
import argparse
from typing import Dict, Tuple, List
from collections import defaultdict

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import Config
from src.generator.tiger_model import TIGERModel
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)


class PlaylistGenerator:
    """å¤„ç†æ¨¡å‹åŠ è½½å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ­Œå•"""

    def __init__(self, config: Config, model_path: str = None):
        """
        åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ä½¿ç”¨æä¾›çš„æ¨¡å‹è·¯å¾„æˆ–é»˜è®¤è·¯å¾„
        if model_path is None:
            model_path = os.path.join(self.config.model_dir, "generator", "final_model")
        self.model_path = model_path
        
        self.model = self._load_model()
        self.semantic_to_song_cluster = self._create_reverse_map()
        self.song_info_map = self._load_song_info()

    def _load_model(self) -> TIGERModel:
        """
        æ™ºèƒ½åŠ è½½TIGERæ¨¡å‹ã€‚
        - å¦‚æœæ˜¯æœ€ç»ˆæ¨¡å‹ç›®å½•ï¼Œåˆ™ä½¿ç”¨ TIGERModel.from_pretrainedã€‚
        - å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ç›®å½•ï¼Œåˆ™é€šè¿‡ TIGERModel.__init__ åŠ è½½ã€‚
        """
        if not os.path.exists(self.model_path):
            logger.error(f"é”™è¯¯: æ¨¡å‹æœªæ‰¾åˆ° {self.model_path}")
            logger.error("è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®")
            sys.exit(1)

        # åˆ¤æ–­æ˜¯å¦ä¸ºæ£€æŸ¥ç‚¹ç›®å½•
        is_checkpoint = "checkpoint" in os.path.basename(os.path.normpath(self.model_path))

        try:
            if is_checkpoint:
                logger.info(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ç›®å½•ï¼Œä½¿ç”¨ __init__ æ–¹æ³•åŠ è½½: {self.model_path}")
                # ä»ä¸»é…ç½®é‡æ–°æ„å»º layer_vocab_sizes
                rq_config = self.config.h_rqkmeans
                layer_vocab_sizes = {
                    'l1': rq_config.need_clusters[0],
                    'l2': rq_config.need_clusters[1],
                    'l3': rq_config.need_clusters[2],
                }
                # ç›´æ¥å®ä¾‹åŒ–TIGERModelã€‚è¿™å°†ä»æ£€æŸ¥ç‚¹åŠ è½½åŸºç¡€T5æ¨¡å‹ï¼Œ
                # ç„¶åé‡æ–°åº”ç”¨è‡ªå®šä¹‰tokenå’ŒåµŒå…¥å±‚å¤§å°è°ƒæ•´ã€‚
                model = TIGERModel(base_model=self.model_path, layer_vocab_sizes=layer_vocab_sizes)
            else:
                logger.info(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨ from_pretrained)...")
                # å¯¹æœ€ç»ˆä¿å­˜çš„æ¨¡å‹ä½¿ç”¨è‡ªå®šä¹‰çš„ from_pretrained æ–¹æ³•
                model = TIGERModel.from_pretrained(self.model_path)
            
            model.model.to(self.device)
            model.model.eval()
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸã€‚è¯æ±‡è¡¨å¤§å°: {len(model.tokenizer)}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            if is_checkpoint:
                logger.error("åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ã€‚è¯·ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å®Œæ•´ï¼Œå¹¶ä¸”Hugging Faceæ¨¡å‹æ–‡ä»¶å­˜åœ¨ã€‚")
            else:
                logger.error("åŠ è½½æœ€ç»ˆæ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®ä¿æ¨¡å‹æ˜¯ä½¿ç”¨ TIGERModel.save_pretrained ä¿å­˜çš„ã€‚")
            sys.exit(1)

    def _create_reverse_map(self) -> Dict[Tuple[int, ...], List[str]]:
        """åˆ›å»ºä»è¯­ä¹‰IDåˆ°æ­Œæ›²IDåˆ—è¡¨çš„åå‘æ˜ å°„"""
        mapping = defaultdict(list)
        semantic_ids_file = os.path.join(self.config.output_dir, "semantic_id", "song_semantic_ids.jsonl")
        if not os.path.exists(semantic_ids_file):
            logger.error(f"é”™è¯¯: song_semantic_ids.jsonl æœªæ‰¾åˆ°äº {semantic_ids_file}")
            logger.error("è¯·å…ˆè¿è¡Œè¯­ä¹‰IDç”Ÿæˆæ­¥éª¤")
            sys.exit(1)

        logger.info("æ­£åœ¨åˆ›å»ºè¯­ä¹‰IDåˆ°æ­Œæ›²ç°‡çš„åå‘æ˜ å°„...")
        with open(semantic_ids_file, 'r', encoding='utf-8') as f:
            for line in f: 
                item = json.loads(line)
                mapping[tuple(item['semantic_ids'])].append(item['song_id'])
        logger.info(f"å·²åŠ è½½ {len(mapping)} ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDç°‡")
        return mapping

    def _load_song_info(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½æ­Œæ›²ä¿¡æ¯ï¼ˆæ­Œåã€æ­Œæ‰‹ï¼‰"""
        import csv
        mapping = {}
        try:
            with open(self.config.data.song_info_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                #next(reader, None)  # è·³è¿‡è¡¨å¤´
                for row in reader:
                    if len(row) >= 3: 
                        mapping[row[0]] = {"name": row[1], "singer": row[2]}
            logger.info(f"å·²åŠ è½½ {len(mapping)} é¦–æ­Œæ›²çš„ä¿¡æ¯")
        except FileNotFoundError: 
            logger.warning(f"æ­Œæ›²ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°: {self.config.data.song_info_file}")
        return mapping

    def generate(self, title: str, tags: str = "", max_songs: int = 20, temperature: float = 0.8) -> List[str]:
        """
        æ ¹æ®æ ‡é¢˜å’Œæ ‡ç­¾ç”Ÿæˆæ­Œå•
        
        Args:
            title: æ­Œå•æ ‡é¢˜/æè¿°
            tags: å¯é€‰æ ‡ç­¾ï¼ˆå½“å‰æœªåœ¨ç”Ÿæˆä¸­ä½¿ç”¨ï¼‰
            max_songs: æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šå¤šæ ·åŒ–ï¼‰
            
        Returns:
            æ­Œæ›²IDåˆ—è¡¨
        """
        # æ ¼å¼åŒ–è¾“å…¥æç¤ºä»¥åŒ¹é…è®­ç»ƒæ ¼å¼
        prompt = title
        logger.info(f"æ­£åœ¨ç”Ÿæˆæ­Œå•ï¼Œæç¤º: '{prompt}'")

        # å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯
        input_ids = self.model.tokenizer.base_tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=self.config.generator_t5.max_input_length,
            truncation=True
        ).input_ids.to(self.device)

        # ç”Ÿæˆè¯­ä¹‰ID
        with torch.no_grad():
            generated_ids = self.model.model.generate(
                input_ids,
                max_new_tokens=self.config.generator_t5.max_target_length,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=temperature,
                num_return_sequences=1,
                pad_token_id=self.model.tokenizer.pad_token_id
            )
        
        # è§£ç ç”Ÿæˆçš„token
        decoded_tokens = self.model.tokenizer.base_tokenizer.convert_ids_to_tokens(
            generated_ids[0], 
            skip_special_tokens=False
        )
        
        logger.debug(f"ç”Ÿæˆçš„token (å‰50ä¸ª): {decoded_tokens[:50]}...")

        # ä»å±‚çº§ç‰¹å®šçš„tokenä¸­æå–è¯­ä¹‰ID
        # æ ¼å¼: <id_l1_X>, <id_l2_Y>, <id_l3_Z>
        semantic_id_tuples = []
        i = 0
        while i < len(decoded_tokens):
            token = decoded_tokens[i]
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¬1å±‚token
            if token.startswith("<id_l1_"):
                # å°è¯•æå–å®Œæ•´çš„3å±‚è¯­ä¹‰ID
                if i + 2 < len(decoded_tokens):
                    l1_token = decoded_tokens[i]
                    l2_token = decoded_tokens[i + 1]
                    l3_token = decoded_tokens[i + 2]
                    
                    # éªŒè¯ä¸‰ä¸ªéƒ½æ˜¯è¯­ä¹‰ID token
                    if (l1_token.startswith("<id_l1_") and 
                        l2_token.startswith("<id_l2_") and 
                        l3_token.startswith("<id_l3_")):
                        try:
                            l1_id = int(l1_token.split('_')[2].rstrip('>'))
                            l2_id = int(l2_token.split('_')[2].rstrip('>'))
                            l3_id = int(l3_token.split('_')[2].rstrip('>'))
                            semantic_id_tuples.append((l1_id, l2_id, l3_id))
                            i += 3
                            continue
                        except (ValueError, IndexError):
                            pass
            i += 1
        
        logger.info(f"æå–äº† {len(semantic_id_tuples)} ä¸ªè¯­ä¹‰IDå…ƒç»„")
        
        # å»é‡åŒæ—¶ä¿æŒé¡ºåº
        unique_semantic_ids = list(dict.fromkeys(semantic_id_tuples))
        logger.info(f"å”¯ä¸€è¯­ä¹‰ID: {len(unique_semantic_ids)}")

        # å¯¹æ¯ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDï¼Œä»å…¶ç°‡ä¸­éšæœºé‡‡æ ·ä¸€é¦–æ­Œ
        reconstructed_song_ids = []
        for id_tuple in unique_semantic_ids:
            if id_tuple in self.semantic_to_song_cluster:
                song_cluster = self.semantic_to_song_cluster[id_tuple]
                # ä»ç°‡ä¸­éšæœºé‡‡æ ·ä¸€é¦–æ­Œ
                sampled_song = random.choice(song_cluster)
                reconstructed_song_ids.append(sampled_song)
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§æ­Œæ›²æ•°åˆ™åœæ­¢
                if len(reconstructed_song_ids) >= max_songs:
                    break
            else:
                logger.debug(f"è¯­ä¹‰ID {id_tuple} åœ¨ç°‡æ˜ å°„ä¸­æœªæ‰¾åˆ°")
        
        logger.info(f"ç”Ÿæˆäº† {len(reconstructed_song_ids)} é¦–æ­Œæ›²")
        return reconstructed_song_ids

    def interactive_demo(self):
        """å¯åŠ¨äº¤äº’å¼å‘½ä»¤è¡Œæ¼”ç¤º"""
        print("\n" + "="*60)
        print("  ğŸµ T5æ­Œå•ç”Ÿæˆæ¨¡å‹ - äº¤äº’å¼æ¼”ç¤º ğŸµ")
        print("="*60)
        print("  è¾“å…¥æ­Œå•æ ‡é¢˜æˆ–æè¿°ï¼Œæ¨¡å‹ä¼šä¸ºæ‚¨ç”Ÿæˆä¸ªæ€§åŒ–æ­Œå•ã€‚")
        print("  æ¨¡å‹ä¼šç”Ÿæˆè¯­ä¹‰IDåºåˆ—ï¼Œç„¶åä»ç›¸ä¼¼æ­Œæ›²ç°‡ä¸­éšæœºé‡‡æ ·ã€‚")
        print("  æ¯æ¬¡ç”Ÿæˆçš„æ­Œå•å¯èƒ½ä¸åŒï¼Œä½“ç°äº†å¤šæ ·æ€§ï¼")
        print("  ")
        print("  å‘½ä»¤:")
        print("    - ç›´æ¥è¾“å…¥æ–‡æœ¬: ç”Ÿæˆæ­Œå•")
        print("    - 'exit' æˆ– 'quit': é€€å‡ºç¨‹åº")
        print("-"*60)

        while True:
            try:
                prompt = input("\nè¯·è¾“å…¥æ­Œå•æ ‡é¢˜/æè¿° > ")
                if prompt.lower() in ['exit', 'quit']:
                    print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                    break
                
                if not prompt.strip(): 
                    continue

                print("\nğŸ¼ ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...")
                song_ids = self.generate(prompt.strip())

                if not song_ids:
                    print("âŒ æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œæ›²åˆ—è¡¨ï¼Œè¯·å°è¯•æ›´æ¢æ ‡é¢˜æˆ–æè¿°ã€‚")
                    continue
                
                print(f"\nâœ¨ ä¸ºæ‚¨æ¨èçš„æ­Œå• (å…±{len(song_ids)}é¦–): âœ¨")
                print("-"*60)
                for i, song_id in enumerate(song_ids, 1):
                    info = self.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                    sem_id = self._get_sem_id_for_song(song_id)
                    cluster_size = len(self.semantic_to_song_cluster.get(sem_id, [])) if sem_id else 0
                    print(f"  {i:2d}. {info['name']} - {info['singer']}")
                    if cluster_size > 1:
                        print(f"      (æ¥è‡ªåŒ…å«{cluster_size}é¦–ç›¸ä¼¼æ­Œæ›²çš„ç°‡)")
                print("-"*60)

            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
                print(f"\nâŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    def _get_sem_id_for_song(self, song_id_to_find: str) -> Tuple[int, ...]:
        """è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾ç»™å®šæ­Œæ›²IDçš„è¯­ä¹‰IDï¼ˆç”¨äºæ˜¾ç¤ºï¼‰"""
        for sem_id, song_list in self.semantic_to_song_cluster.items():
            if song_id_to_find in song_list:
                return sem_id
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--model_path", 
        type=str, 
        default=None,
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: models/generator/final_model)"
    )
    parser.add_argument(
        "--prompt", 
        type=str, 
        default=None,
        help="ç›´æ¥ç”Ÿæˆæ­Œå•çš„æç¤ºæ–‡æœ¬ (å¦‚æœä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼)"
    )
    parser.add_argument(
        "--max_songs", 
        type=int, 
        default=20,
        help="æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡ (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.8,
        help="é‡‡æ ·æ¸©åº¦ï¼Œè¶Šé«˜è¶Šå¤šæ ·åŒ– (é»˜è®¤: 0.8)"
    )
    parser.add_argument(
        "--log_level", 
        type=str, 
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level)
    setup_logging(level=log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨...")
    generator = PlaylistGenerator(config, model_path=args.model_path)
    
    # ç”Ÿæˆæˆ–å¯åŠ¨äº¤äº’æ¨¡å¼
    if args.prompt:
        # å•æ¬¡ç”Ÿæˆæ¨¡å¼
        logger.info(f"æ­£åœ¨ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆæ­Œå•: '{args.prompt}'")
        song_ids = generator.generate(
            args.prompt, 
            max_songs=args.max_songs,
            temperature=args.temperature
        )
        
        if song_ids:
            print(f"\nç”Ÿæˆçš„æ­Œå• (å…±{len(song_ids)}é¦–):")
            print("="*60)
            for i, song_id in enumerate(song_ids, 1):
                info = generator.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                print(f"{i:2d}. {info['name']} - {info['singer']}")
            print("="*60)
        else:
            print("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œå•ï¼Œè¯·å°è¯•å…¶ä»–æç¤ºæ–‡æœ¬ã€‚")
    else:
        # äº¤äº’æ¨¡å¼
        generator.interactive_demo()
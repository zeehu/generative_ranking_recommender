"""
T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†è„šæœ¬       
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„T5æ¨¡å‹å¹¶æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ­Œæ›²æ¨è

æ”¯æŒä¸¤ç§æ¨¡å‹åŠ è½½æ–¹å¼:
1. ä»è®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½ (checkpointç›®å½•)
   - åŒ…å«æ–‡ä»¶: config.json, model.safetensors, generation_config.jsonç­‰
   - è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½æ£€æŸ¥ç‚¹
   
2. ä»æœ€ç»ˆä¿å­˜çš„æ¨¡å‹åŠ è½½ (final_modelç›®å½•)
   - ä½¿ç”¨ TIGERModel.save_pretrained() ä¿å­˜çš„æ¨¡å‹
   - åŒ…å«å®Œæ•´çš„æ¨¡å‹é…ç½®å’Œæƒé‡

ä½¿ç”¨ç¤ºä¾‹:
---------
1. ä»æ£€æŸ¥ç‚¹åŠ è½½å¹¶è¿›å…¥äº¤äº’æ¨¡å¼:
   python src/generator/inference_t5.py --model_path models/generator/checkpoint-1000

2. ä»æœ€ç»ˆæ¨¡å‹åŠ è½½å¹¶ç”Ÿæˆå•ä¸ªæ­Œå•:
   python src/generator/inference_t5.py --model_path models/generator/final_model --prompt "é€‚åˆè¿åŠ¨çš„æ­Œæ›²"

3. è°ƒæ•´ç”Ÿæˆå‚æ•°:
   python src/generator/inference_t5.py --model_path models/generator/checkpoint-1000 --max_songs 30 --temperature 1.0

4. å¯ç”¨è°ƒè¯•æ—¥å¿—:
   python src/generator/inference_t5.py --model_path models/generator/checkpoint-1000 --log_level DEBUG
"""
import os
import sys
import torch
import json
import logging
import random
import argparse
from typing import Dict, Tuple, List
from collections import defaultdict

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config_optimized import Config
from src.generator.tiger_model import TIGERModel
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)


class PlaylistGenerator:
    """å¤„ç†æ¨¡å‹åŠ è½½å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ­Œå•"""

    def __init__(self, config: Config, model_path: str = None):
        """
        åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ä½¿ç”¨æä¾›çš„æ¨¡å‹è·¯å¾„æˆ–é»˜è®¤è·¯å¾„
        if model_path is None:
            model_path = os.path.join(self.config.model_dir, "generator", "final_model")
        self.model_path = model_path
        
        self.model = self._load_model()
        self.semantic_to_song_cluster = self._create_reverse_map()
        self.song_info_map = self._load_song_info()

    def _load_model(self) -> TIGERModel:
        """
        æ™ºèƒ½åŠ è½½TIGERæ¨¡å‹ã€‚
        - å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆåŒ…å«model.safetensorsï¼‰ï¼Œåˆ™ä»æ£€æŸ¥ç‚¹åŠ è½½ã€‚
        - å¦‚æœæ˜¯æœ€ç»ˆæ¨¡å‹ç›®å½•ï¼Œåˆ™ä½¿ç”¨ TIGERModel.from_pretrainedã€‚
        """
        if not os.path.exists(self.model_path):
            logger.error(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ {self.model_path}")
            logger.error("è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®")
            sys.exit(1)

        # æ£€æŸ¥ç›®å½•ä¸­çš„å…³é”®æ–‡ä»¶ä»¥åˆ¤æ–­æ¨¡å‹ç±»å‹
        is_checkpoint = self._is_checkpoint_dir(self.model_path)

        try:
            if is_checkpoint:
                logger.info(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ç›®å½•ï¼Œæ­£åœ¨åŠ è½½: {self.model_path}")
                logger.info(f"æ£€æŸ¥ç‚¹åŒ…å«æ–‡ä»¶: {os.listdir(self.model_path)}")
                
                # ä»ä¸»é…ç½®é‡æ–°æ„å»º layer_vocab_sizes
                rq_config = self.config.h_rqkmeans
                layer_vocab_sizes = {
                    'l1': rq_config.need_clusters[0],
                    'l2': rq_config.need_clusters[1],
                    'l3': rq_config.need_clusters[2],
                }
                logger.info(f"ä½¿ç”¨å±‚çº§è¯æ±‡è¡¨å¤§å°: {layer_vocab_sizes}")
                
                # æ£€æŸ¥checkpointæ˜¯å¦åŒ…å«tokenizeræ–‡ä»¶
                has_tokenizer = self._has_tokenizer_files(self.model_path)
                
                if not has_tokenizer:
                    logger.warning("æ£€æŸ¥ç‚¹ç›®å½•ç¼ºå°‘tokenizeræ–‡ä»¶ï¼ˆspiece.modelç­‰ï¼‰")
                    logger.info(f"å°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è·¯å¾„åŠ è½½tokenizer: {self.config.generator_t5.model_name}")
                    # ä½¿ç”¨ä¿®æ”¹åçš„åŠ è½½æ–¹å¼
                    model = self._load_from_checkpoint_without_tokenizer(
                        self.model_path, 
                        self.config.generator_t5.model_name,
                        layer_vocab_sizes
                    )
                else:
                    # ç›´æ¥å®ä¾‹åŒ–TIGERModelï¼Œè¿™å°†ä»æ£€æŸ¥ç‚¹åŠ è½½åŸºç¡€T5æ¨¡å‹
                    # ç„¶åé‡æ–°åº”ç”¨è‡ªå®šä¹‰tokenå’ŒåµŒå…¥å±‚å¤§å°è°ƒæ•´
                    model = TIGERModel(base_model=self.model_path, layer_vocab_sizes=layer_vocab_sizes)
                
                logger.info("æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
            else:
                logger.info(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨ from_pretrained)...")
                # å¯¹æœ€ç»ˆä¿å­˜çš„æ¨¡å‹ä½¿ç”¨è‡ªå®šä¹‰çš„ from_pretrained æ–¹æ³•
                model = TIGERModel.from_pretrained(self.model_path)
                logger.info("æœ€ç»ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            model.model.to(self.device)
            model.model.eval()
            logger.info(f"æ¨¡å‹å·²ç§»è‡³è®¾å¤‡ {self.device} å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
            logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(model.tokenizer)}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            if is_checkpoint:
                logger.error("åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ã€‚å¯èƒ½çš„åŸå› :")
                logger.error("  1. ç¼ºå°‘å¿…éœ€æ–‡ä»¶: config.json, model.safetensors")
                logger.error("  2. ç¼ºå°‘tokenizeræ–‡ä»¶: spiece.model, tokenizer.jsonç­‰")
                logger.error("  3. protobufåº“æœªå®‰è£…: pip install protobuf sentencepiece")
                logger.error(f"\nè¯·æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•: {self.model_path}")
            else:
                logger.error("åŠ è½½æœ€ç»ˆæ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®ä¿æ¨¡å‹æ˜¯ä½¿ç”¨ TIGERModel.save_pretrained ä¿å­˜çš„ã€‚")
            sys.exit(1)
    
    def _has_tokenizer_files(self, path: str) -> bool:
        """
        æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ…å«tokenizeræ–‡ä»¶ã€‚
        
        Args:
            path: è¦æ£€æŸ¥çš„ç›®å½•è·¯å¾„
            
        Returns:
            å¦‚æœåŒ…å«tokenizeræ–‡ä»¶è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not os.path.isdir(path):
            return False
        
        files = os.listdir(path)
        
        # T5 tokenizeréœ€è¦çš„æ–‡ä»¶
        tokenizer_files = [
            'spiece.model',           # SentencePieceæ¨¡å‹æ–‡ä»¶ï¼ˆå¿…éœ€ï¼‰
            'tokenizer.json',         # æˆ–è€…tokenizeré…ç½®
            'tokenizer_config.json',  # tokenizeré…ç½®
        ]
        
        # è‡³å°‘éœ€è¦spiece.model
        has_spiece = 'spiece.model' in files
        
        if has_spiece:
            logger.debug(f"ç›®å½• {path} åŒ…å«tokenizeræ–‡ä»¶")
        else:
            logger.debug(f"ç›®å½• {path} ç¼ºå°‘tokenizeræ–‡ä»¶")
        
        return has_spiece
    
    def _load_from_checkpoint_without_tokenizer(self, checkpoint_path: str, 
                                                base_model_path: str,
                                                layer_vocab_sizes: dict) -> TIGERModel:
        """
        ä»ç¼ºå°‘tokenizeræ–‡ä»¶çš„checkpointåŠ è½½æ¨¡å‹ã€‚
        ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„tokenizerï¼Œç„¶ååŠ è½½checkpointçš„æƒé‡ã€‚
        
        Args:
            checkpoint_path: checkpointç›®å½•è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºåŠ è½½tokenizerï¼‰
            layer_vocab_sizes: å±‚çº§è¯æ±‡è¡¨å¤§å°
            
        Returns:
            åŠ è½½å¥½çš„TIGERModel
        """
        from transformers import T5ForConditionalGeneration
        from src.generator.tiger_model import TIGERTokenizer
        
        logger.info(f"ä»åŸºç¡€æ¨¡å‹åŠ è½½tokenizer: {base_model_path}")
        
        # åˆ›å»ºTIGERæ¨¡å‹å®ä¾‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹çš„tokenizer
        tiger_model = TIGERModel.__new__(TIGERModel)
        super(TIGERModel, tiger_model).__init__()
        
        # åˆå§‹åŒ–tokenizerï¼ˆä»åŸºç¡€æ¨¡å‹ï¼‰
        tiger_model.tokenizer = TIGERTokenizer(base_model_path, layer_vocab_sizes)
        tiger_model.layer_vocab_sizes = layer_vocab_sizes
        tiger_model.base_model_path = base_model_path
        
        # ä»checkpointåŠ è½½T5æ¨¡å‹
        logger.info(f"ä»checkpointåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
        tiger_model.model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)
        tiger_model.config = tiger_model.model.config
        
        # éªŒè¯è¯æ±‡è¡¨å¤§å°
        expected_vocab_size = len(tiger_model.tokenizer)
        actual_vocab_size = tiger_model.model.config.vocab_size
        
        if actual_vocab_size != expected_vocab_size:
            logger.warning(
                f"è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: æ¨¡å‹={actual_vocab_size}, tokenizer={expected_vocab_size}"
            )
            logger.info("è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°ä»¥åŒ¹é…tokenizer...")
            tiger_model.model.resize_token_embeddings(expected_vocab_size)
        
        logger.info(f"æˆåŠŸåŠ è½½checkpointï¼Œè¯æ±‡è¡¨å¤§å°: {len(tiger_model.tokenizer)}")
        
        return tiger_model
    
    def _is_checkpoint_dir(self, path: str) -> bool:
        """
        åˆ¤æ–­ç»™å®šè·¯å¾„æ˜¯å¦ä¸ºè®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½•ã€‚
        æ£€æŸ¥ç‚¹ç›®å½•é€šå¸¸åŒ…å«: model.safetensors, config.json, optimizer.pt, scheduler.pt ç­‰ã€‚
        
        Args:
            path: è¦æ£€æŸ¥çš„ç›®å½•è·¯å¾„
            
        Returns:
            å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ç›®å½•è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not os.path.isdir(path):
            return False
        
        files = os.listdir(path)
        
        # æ£€æŸ¥ç‚¹ç›®å½•çš„ç‰¹å¾æ–‡ä»¶
        checkpoint_indicators = [
            'model.safetensors',      # Hugging Face safetensorsæ ¼å¼
            'pytorch_model.bin',      # æˆ–ä¼ ç»Ÿçš„PyTorchæ ¼å¼
            'optimizer.pt',           # ä¼˜åŒ–å™¨çŠ¶æ€
            'scheduler.pt',           # è°ƒåº¦å™¨çŠ¶æ€
            'trainer_state.json',     # è®­ç»ƒå™¨çŠ¶æ€
        ]
        
        # å¦‚æœåŒ…å«ä»»ä½•æ£€æŸ¥ç‚¹ç‰¹å¾æ–‡ä»¶ï¼Œåˆ™è®¤ä¸ºæ˜¯æ£€æŸ¥ç‚¹ç›®å½•
        has_checkpoint_files = any(f in files for f in checkpoint_indicators)
        
        # åŒæ—¶æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…éœ€çš„æ¨¡å‹é…ç½®æ–‡ä»¶
        has_config = 'config.json' in files
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æƒé‡æ–‡ä»¶
        has_model_weights = 'model.safetensors' in files or 'pytorch_model.bin' in files
        
        is_checkpoint = has_checkpoint_files and has_config and has_model_weights
        
        if is_checkpoint:
            logger.debug(f"ç›®å½• {path} è¢«è¯†åˆ«ä¸ºæ£€æŸ¥ç‚¹ç›®å½•")
            logger.debug(f"åŒ…å«æ–‡ä»¶: {files}")
        
        return is_checkpoint

    def _create_reverse_map(self) -> Dict[Tuple[int, ...], List[str]]:
        """åˆ›å»ºä»è¯­ä¹‰IDåˆ°æ­Œæ›²IDåˆ—è¡¨çš„åå‘æ˜ å°„"""
        mapping = defaultdict(list)
        semantic_ids_file = os.path.join(self.config.output_dir, "semantic_id", "song_semantic_ids.jsonl")
        if not os.path.exists(semantic_ids_file):
            logger.error(f"é”™è¯¯: song_semantic_ids.jsonl æœªæ‰¾åˆ°äº {semantic_ids_file}")
            logger.error("è¯·å…ˆè¿è¡Œè¯­ä¹‰IDç”Ÿæˆæ­¥éª¤")
            sys.exit(1)

        logger.info("æ­£åœ¨åˆ›å»ºè¯­ä¹‰IDåˆ°æ­Œæ›²ç°‡çš„åå‘æ˜ å°„...")
        with open(semantic_ids_file, 'r', encoding='utf-8') as f:
            for line in f: 
                item = json.loads(line)
                mapping[tuple(item['semantic_ids'])].append(item['song_id'])
        logger.info(f"å·²åŠ è½½ {len(mapping)} ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDç°‡")
        return mapping

    def _load_song_info(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½æ­Œæ›²ä¿¡æ¯ï¼ˆæ­Œåã€æ­Œæ‰‹ï¼‰"""
        import csv
        mapping = {}
        try:
            with open(self.config.data.song_info_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                #next(reader, None)  # è·³è¿‡è¡¨å¤´
                for row in reader:
                    if len(row) >= 3: 
                        mapping[row[0]] = {"name": row[1], "singer": row[2]}
            logger.info(f"å·²åŠ è½½ {len(mapping)} é¦–æ­Œæ›²çš„ä¿¡æ¯")
        except FileNotFoundError: 
            logger.warning(f"æ­Œæ›²ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°: {self.config.data.song_info_file}")
        return mapping

    def generate(self, title: str, tags: str = "", max_songs: int = 20, temperature: float = 0.8) -> List[Dict]:
        """
        æ ¹æ®æ ‡é¢˜å’Œæ ‡ç­¾ç”Ÿæˆæ­Œå•
        
        Args:
            title: æ­Œå•æ ‡é¢˜/æè¿°
            tags: å¯é€‰æ ‡ç­¾ï¼ˆå½“å‰æœªåœ¨ç”Ÿæˆä¸­ä½¿ç”¨ï¼‰
            max_songs: æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šå¤šæ ·åŒ–ï¼‰
            
        Returns:
            æ­Œæ›²ä¿¡æ¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«:
            - song_id: æ­Œæ›²ID
            - semantic_id: è¯­ä¹‰IDå…ƒç»„
            - cluster_songs: ç°‡ä¸­çš„æ‰€æœ‰æ­Œæ›²IDåˆ—è¡¨
        """
        # æ ¼å¼åŒ–è¾“å…¥æç¤ºä»¥åŒ¹é…è®­ç»ƒæ ¼å¼
        prompt = title
        logger.info(f"æ­£åœ¨ç”Ÿæˆæ­Œå•ï¼Œæç¤º: '{prompt}'")

        # å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯
        input_ids = self.model.tokenizer.base_tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=self.config.generator_t5.max_input_length,
            truncation=True
        ).input_ids.to(self.device)

        # ç”Ÿæˆè¯­ä¹‰ID
        with torch.no_grad():
            generated_ids = self.model.model.generate(
                input_ids,
                max_new_tokens=self.config.generator_t5.max_target_length,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=temperature,
                num_return_sequences=1,
                pad_token_id=self.model.tokenizer.pad_token_id
            )
        
        # è§£ç ç”Ÿæˆçš„token
        decoded_tokens = self.model.tokenizer.base_tokenizer.convert_ids_to_tokens(
            generated_ids[0], 
            skip_special_tokens=False
        )
        
        logger.debug(f"ç”Ÿæˆçš„token (å‰50ä¸ª): {decoded_tokens[:50]}...")

        # ä»å±‚çº§ç‰¹å®šçš„tokenä¸­æå–è¯­ä¹‰ID
        # æ ¼å¼: <id_l1_X>, <id_l2_Y>, <id_l3_Z>
        semantic_id_tuples = []
        i = 0
        while i < len(decoded_tokens):
            token = decoded_tokens[i]
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¬1å±‚token
            if token.startswith("<id_l1_"):
                # å°è¯•æå–å®Œæ•´çš„3å±‚è¯­ä¹‰ID
                if i + 2 < len(decoded_tokens):
                    l1_token = decoded_tokens[i]
                    l2_token = decoded_tokens[i + 1]
                    l3_token = decoded_tokens[i + 2]
                    
                    # éªŒè¯ä¸‰ä¸ªéƒ½æ˜¯è¯­ä¹‰ID token
                    if (l1_token.startswith("<id_l1_") and 
                        l2_token.startswith("<id_l2_") and 
                        l3_token.startswith("<id_l3_")):
                        try:
                            l1_id = int(l1_token.split('_')[2].rstrip('>'))
                            l2_id = int(l2_token.split('_')[2].rstrip('>'))
                            l3_id = int(l3_token.split('_')[2].rstrip('>'))
                            semantic_id_tuples.append((l1_id, l2_id, l3_id))
                            i += 3
                            continue
                        except (ValueError, IndexError):
                            pass
            i += 1
        
        logger.info(f"æå–äº† {len(semantic_id_tuples)} ä¸ªè¯­ä¹‰IDå…ƒç»„")
        
        # å»é‡åŒæ—¶ä¿æŒé¡ºåº
        unique_semantic_ids = list(dict.fromkeys(semantic_id_tuples))
        logger.info(f"å”¯ä¸€è¯­ä¹‰ID: {len(unique_semantic_ids)}")
        
        # DEBUG: æ‰“å°å…¨éƒ¨å”¯ä¸€è¯­ä¹‰IDåŠå…¶ç”Ÿæˆæ¬¡æ•°
        if logger.isEnabledFor(logging.DEBUG):
            from collections import Counter
            semantic_id_counts = Counter(semantic_id_tuples)
            
            logger.debug("\n" + "="*100)
            logger.debug("å…¨éƒ¨å”¯ä¸€è¯­ä¹‰IDåºåˆ—åŠå…¶ç”Ÿæˆæ¬¡æ•°:")
            logger.debug("="*100)
            
            # æŒ‰ç”Ÿæˆæ¬¡æ•°ä»é«˜åˆ°ä½æ’åº
            sorted_ids = sorted(semantic_id_counts.items(), key=lambda x: x[1], reverse=True)
            
            for rank, (sem_id, count) in enumerate(sorted_ids, 1):
                cluster_size = len(self.semantic_to_song_cluster.get(sem_id, []))
                status = "âœ“" if sem_id in self.semantic_to_song_cluster else "âœ—"
                logger.debug(
                    f"{rank:3d}. è¯­ä¹‰ID: ({sem_id[0]:3d}, {sem_id[1]:3d}, {sem_id[2]:3d}) | "
                    f"ç”Ÿæˆæ¬¡æ•°: {count:3d} | ç°‡å¤§å°: {cluster_size:3d} | {status}"
                )
            
            logger.debug("="*100)
            logger.debug(f"ç»Ÿè®¡ä¿¡æ¯:")
            logger.debug(f"  - æ€»ç”Ÿæˆæ¬¡æ•°: {len(semantic_id_tuples)}")
            logger.debug(f"  - å”¯ä¸€è¯­ä¹‰IDæ•°: {len(unique_semantic_ids)}")
            logger.debug(f"  - æœ‰æ•ˆè¯­ä¹‰IDæ•°: {sum(1 for sem_id in unique_semantic_ids if sem_id in self.semantic_to_song_cluster)}")
            logger.debug(f"  - æ— æ•ˆè¯­ä¹‰IDæ•°: {sum(1 for sem_id in unique_semantic_ids if sem_id not in self.semantic_to_song_cluster)}")
            logger.debug("="*100 + "\n")

        # å¯¹æ¯ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDï¼Œä»å…¶ç°‡ä¸­éšæœºé‡‡æ ·ä¸€é¦–æ­Œï¼Œå¹¶ä¿å­˜å®Œæ•´ä¿¡æ¯
        reconstructed_songs = []
        for id_tuple in unique_semantic_ids:
            if id_tuple in self.semantic_to_song_cluster:
                song_cluster = self.semantic_to_song_cluster[id_tuple]
                # ä»ç°‡ä¸­éšæœºé‡‡æ ·ä¸€é¦–æ­Œ
                sampled_song = random.choice(song_cluster)
                
                # ä¿å­˜æ­Œæ›²ä¿¡æ¯
                song_info = {
                    'song_id': sampled_song,
                    'semantic_id': id_tuple,
                    'cluster_songs': song_cluster
                }
                reconstructed_songs.append(song_info)
                
                # å¦‚æœè¾¾åˆ°æœ€å¤§æ­Œæ›²æ•°åˆ™åœæ­¢
                if len(reconstructed_songs) >= max_songs:
                    break
            else:
                logger.debug(f"è¯­ä¹‰ID {id_tuple} åœ¨ç°‡æ˜ å°„ä¸­æœªæ‰¾åˆ°")
        
        logger.info(f"ç”Ÿæˆäº† {len(reconstructed_songs)} é¦–æ­Œæ›²")
        return reconstructed_songs

    def interactive_demo(self):
        """å¯åŠ¨äº¤äº’å¼å‘½ä»¤è¡Œæ¼”ç¤º"""
        print("\n" + "="*60)
        print("  ğŸµ T5æ­Œå•ç”Ÿæˆæ¨¡å‹ - äº¤äº’å¼æ¼”ç¤º ğŸµ")
        print("="*60)
        print("  è¾“å…¥æ­Œå•æ ‡é¢˜æˆ–æè¿°ï¼Œæ¨¡å‹ä¼šä¸ºæ‚¨ç”Ÿæˆä¸ªæ€§åŒ–æ­Œå•ã€‚")
        print("  æ¨¡å‹ä¼šç”Ÿæˆè¯­ä¹‰IDåºåˆ—ï¼Œç„¶åä»ç›¸ä¼¼æ­Œæ›²ç°‡ä¸­éšæœºé‡‡æ ·ã€‚")
        print("  æ¯æ¬¡ç”Ÿæˆçš„æ­Œå•å¯èƒ½ä¸åŒï¼Œä½“ç°äº†å¤šæ ·æ€§ï¼")
        print("  ")
        print("  å‘½ä»¤:")
        print("    - ç›´æ¥è¾“å…¥æ–‡æœ¬: ç”Ÿæˆæ­Œå•")
        print("    - 'exit' æˆ– 'quit': é€€å‡ºç¨‹åº")
        print("-"*60)

        while True:
            try:
                prompt = input("\nè¯·è¾“å…¥æ­Œå•æ ‡é¢˜/æè¿° > ")
                if prompt.lower() in ['exit', 'quit']:
                    print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                    break
                
                if not prompt.strip(): 
                    continue

                print("\nğŸ¼ ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...")
                songs = self.generate(prompt.strip())

                if not songs:
                    print("âŒ æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œæ›²åˆ—è¡¨ï¼Œè¯·å°è¯•æ›´æ¢æ ‡é¢˜æˆ–æè¿°ã€‚")
                    continue
                
                print(f"\nâœ¨ ä¸ºæ‚¨æ¨èçš„æ­Œå• (å…±{len(songs)}é¦–): âœ¨")
                print("="*100)
                
                for i, song_data in enumerate(songs, 1):
                    song_id = song_data['song_id']
                    sem_id = song_data['semantic_id']
                    cluster_songs = song_data['cluster_songs']
                    
                    info = self.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                    
                    # æ„å»ºä¸»æ­Œæ›²ä¿¡æ¯ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
                    main_song = f"{i:2d}. {info['name']} - {info['singer']} - {song_id} - {sem_id[0]}, {sem_id[1]}, {sem_id[2]}"
                    
                    # å¦‚æœç°‡ä¸­æœ‰å¤šé¦–æ­Œæ›²ï¼Œæ·»åŠ ç°‡ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤º4é¦–å…¶ä»–æ­Œæ›²ï¼‰
                    if len(cluster_songs) > 1:
                        other_songs = [s for s in cluster_songs if s != song_id]
                        cluster_info_parts = []
                        
                        for other_song_id in other_songs[:4]:
                            other_info = self.song_info_map.get(other_song_id, {"name": "æœªçŸ¥", "singer": "æœªçŸ¥"})
                            # è·å–è¯¥æ­Œæ›²çš„è¯­ä¹‰IDï¼ˆåº”è¯¥å’Œä¸»æ­Œæ›²ç›¸åŒï¼‰
                            cluster_info_parts.append(f"{other_info['name']} - {other_info['singer']} - {other_song_id}")
                        
                        if cluster_info_parts:
                            cluster_str = "; ".join(cluster_info_parts)
                            if len(other_songs) > 4:
                                cluster_str += f"; ... è¿˜æœ‰{len(other_songs)-4}é¦–"
                            print(f"{main_song} ({cluster_str})")
                        else:
                            print(main_song)
                    else:
                        print(main_song)
                
                print("="*100)

            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
                print(f"\nâŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    def _get_sem_id_for_song(self, song_id_to_find: str) -> Tuple[int, ...]:
        """è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾ç»™å®šæ­Œæ›²IDçš„è¯­ä¹‰IDï¼ˆç”¨äºæ˜¾ç¤ºï¼‰"""
        for sem_id, song_list in self.semantic_to_song_cluster.items():
            if song_id_to_find in song_list:
                return sem_id
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--model_path", 
        type=str, 
        default=None,
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: models/generator/final_model)"
    )
    parser.add_argument(
        "--prompt", 
        type=str, 
        default=None,
        help="ç›´æ¥ç”Ÿæˆæ­Œå•çš„æç¤ºæ–‡æœ¬ (å¦‚æœä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼)"
    )
    parser.add_argument(
        "--max_songs", 
        type=int, 
        default=20,
        help="æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡ (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.8,
        help="é‡‡æ ·æ¸©åº¦ï¼Œè¶Šé«˜è¶Šå¤šæ ·åŒ– (é»˜è®¤: 0.8)"
    )
    parser.add_argument(
        "--log_level", 
        type=str, 
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level)
    setup_logging(level=log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨...")
    generator = PlaylistGenerator(config, model_path=args.model_path)
    
    # ç”Ÿæˆæˆ–å¯åŠ¨äº¤äº’æ¨¡å¼
    if args.prompt:
        # å•æ¬¡ç”Ÿæˆæ¨¡å¼
        logger.info(f"æ­£åœ¨ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆæ­Œå•: '{args.prompt}'")
        songs = generator.generate(
            args.prompt, 
            max_songs=args.max_songs,
            temperature=args.temperature
        )
        
        if songs:
            print(f"\nç”Ÿæˆçš„æ­Œå• (å…±{len(songs)}é¦–):")
            print("="*100)
            
            for i, song_data in enumerate(songs, 1):
                song_id = song_data['song_id']
                sem_id = song_data['semantic_id']
                cluster_songs = song_data['cluster_songs']
                
                info = generator.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                
                # æ„å»ºä¸»æ­Œæ›²ä¿¡æ¯ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
                main_song = f"{i:2d}. {info['name']} - {info['singer']} - {song_id} - {sem_id[0]}, {sem_id[1]}, {sem_id[2]}"
                
                # å¦‚æœç°‡ä¸­æœ‰å¤šé¦–æ­Œæ›²ï¼Œæ·»åŠ ç°‡ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤º4é¦–å…¶ä»–æ­Œæ›²ï¼‰
                if len(cluster_songs) > 1:
                    other_songs = [s for s in cluster_songs if s != song_id]
                    cluster_info_parts = []
                    
                    for other_song_id in other_songs[:4]:
                        other_info = generator.song_info_map.get(other_song_id, {"name": "æœªçŸ¥", "singer": "æœªçŸ¥"})
                        # è·å–è¯¥æ­Œæ›²çš„è¯­ä¹‰IDï¼ˆåº”è¯¥å’Œä¸»æ­Œæ›²ç›¸åŒï¼‰
                        cluster_info_parts.append(f"{other_info['name']} - {other_info['singer']} - {other_song_id}")
                    
                    if cluster_info_parts:
                        cluster_str = "; ".join(cluster_info_parts)
                        if len(other_songs) > 4:
                            cluster_str += f"; ... è¿˜æœ‰{len(other_songs)-4}é¦–"
                        print(f"{main_song} ({cluster_str})")
                    else:
                        print(main_song)
                else:
                    print(main_song)
            
            print("="*100)
        else:
            print("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œå•ï¼Œè¯·å°è¯•å…¶ä»–æç¤ºæ–‡æœ¬ã€‚")
    else:
        # äº¤äº’æ¨¡å¼
        generator.interactive_demo()
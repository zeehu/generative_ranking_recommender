""" 
T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„T5æ¨¡å‹å¹¶æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ­Œæ›²æ¨è
"""
import os
import sys
import torch
import json
import logging
import random
import argparse
from typing import Dict, Tuple, List
from collections import defaultdict

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import Config
from src.generator.tiger_model import TIGERModel
from src.generator.semantic_id_trie import SemanticIDTrie, ConstrainedLogitsProcessor
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)


class PlaylistGenerator:
    """å¤„ç†æ¨¡å‹åŠ è½½å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ­Œå•"""

    def __init__(self, config: Config, model_path: str = None, use_trie_constraint: bool = True):
        """
        åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            use_trie_constraint: æ˜¯å¦ä½¿ç”¨Trieæ ‘çº¦æŸç”Ÿæˆï¼ˆé»˜è®¤Trueï¼‰
        """
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if model_path is None:
            model_path = os.path.join(self.config.model_dir, "generator", "final_model")
        self.model_path = model_path
        
        self.model = self._load_model()
        self.semantic_to_song_cluster = self._create_reverse_map()
        self.song_info_map = self._load_song_info()
        
        # åˆå§‹åŒ–Trieæ ‘çº¦æŸï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.use_trie_constraint = use_trie_constraint
        self.trie = None
        self.constrained_processor = None
        
        if self.use_trie_constraint:
            self._init_trie_constraint()

    def _load_model(self) -> TIGERModel:
        """
        æ™ºèƒ½åŠ è½½TIGERæ¨¡å‹ã€‚
        - å¦‚æœæ˜¯æœ€ç»ˆæ¨¡å‹ç›®å½•ï¼Œåˆ™ä½¿ç”¨ TIGERModel.from_pretrainedã€‚
        - å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ç›®å½•ï¼Œåˆ™é€šè¿‡ TIGERModel.__init__ åŠ è½½ã€‚
        """
        if not os.path.exists(self.model_path):
            logger.error(f"é”™è¯¯: æ¨¡å‹æœªæ‰¾åˆ° {self.model_path}")
            logger.error("è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®")
            sys.exit(1)

        is_checkpoint = "checkpoint" in os.path.basename(os.path.normpath(self.model_path))

        try:
            if is_checkpoint:
                logger.info(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ç›®å½•ï¼Œä½¿ç”¨ __init__ æ–¹æ³•åŠ è½½: {self.model_path}")
                rq_config = self.config.h_rqkmeans
                layer_vocab_sizes = {
                    'l1': rq_config.need_clusters[0],
                    'l2': rq_config.need_clusters[1],
                    'l3': rq_config.need_clusters[2],
                }
                model = TIGERModel(base_model=self.model_path, layer_vocab_sizes=layer_vocab_sizes)
            else:
                logger.info(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨ from_pretrained)...")
                model = TIGERModel.from_pretrained(self.model_path)
            
            model.model.to(self.device)
            model.model.eval()
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸã€‚è¯æ±‡è¡¨å¤§å°: {len(model.tokenizer)}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            if is_checkpoint:
                logger.error("åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ã€‚è¯·ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å®Œæ•´ï¼Œå¹¶ä¸”Hugging Faceæ¨¡å‹æ–‡ä»¶å­˜åœ¨ã€‚")
            else:
                logger.error("åŠ è½½æœ€ç»ˆæ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®ä¿æ¨¡å‹æ˜¯ä½¿ç”¨ TIGERModel.save_pretrained ä¿å­˜çš„ã€‚")
            sys.exit(1)

    def _create_reverse_map(self) -> Dict[Tuple[int, ...], List[str]]:
        """åˆ›å»ºä»è¯­ä¹‰IDåˆ°æ­Œæ›²IDåˆ—è¡¨çš„åå‘æ˜ å°„"""
        mapping = defaultdict(list)
        semantic_ids_file = os.path.join(self.config.output_dir, "semantic_id", "song_semantic_ids.jsonl")
        if not os.path.exists(semantic_ids_file):
            logger.error(f"é”™è¯¯: song_semantic_ids.jsonl æœªæ‰¾åˆ°äº {semantic_ids_file}")
            logger.error("è¯·å…ˆè¿è¡Œè¯­ä¹‰IDç”Ÿæˆæ­¥éª¤")
            sys.exit(1)

        logger.info("æ­£åœ¨åˆ›å»ºè¯­ä¹‰IDåˆ°æ­Œæ›²ç°‡çš„åå‘æ˜ å°„...")
        with open(semantic_ids_file, 'r', encoding='utf-8') as f:
            for line in f: 
                item = json.loads(line)
                mapping[tuple(item['semantic_ids'])].append(item['song_id'])
        logger.info(f"å·²åŠ è½½ {len(mapping)} ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDç°‡")
        return mapping

    def _load_song_info(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½æ­Œæ›²ä¿¡æ¯ï¼ˆæ­Œåã€æ­Œæ‰‹ï¼‰"""
        import csv
        mapping = {}
        try:
            with open(self.config.data.song_info_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                for row in reader:
                    if len(row) >= 3: 
                        mapping[row[0]] = {"name": row[1], "singer": row[2]}
            logger.info(f"å·²åŠ è½½ {len(mapping)} é¦–æ­Œæ›²çš„ä¿¡æ¯")
        except FileNotFoundError: 
            logger.warning(f"æ­Œæ›²ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°: {self.config.data.song_info_file}")
        return mapping
    
    def _init_trie_constraint(self):
        """åˆå§‹åŒ–Trieæ ‘çº¦æŸ"""
        try:
            semantic_ids_file = os.path.join(self.config.output_dir, "semantic_id", "song_semantic_ids.jsonl")
            logger.info("æ­£åœ¨åˆå§‹åŒ–Trieæ ‘çº¦æŸ...")
            
            # æ„å»ºTrieæ ‘
            self.trie = SemanticIDTrie(self.model.tokenizer, semantic_ids_file)
            
            # åˆ›å»ºçº¦æŸå¤„ç†å™¨
            self.constrained_processor = ConstrainedLogitsProcessor(
                self.trie, 
                self.model.tokenizer, 
                self.model.tokenizer.eos_token_id
            )
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            stats = self.trie.get_statistics()
            logger.info(f"Trieæ ‘ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"  - æœ‰æ•ˆè¯­ä¹‰IDåºåˆ—æ€»æ•°: {stats['total_valid_sequences']}")
            logger.info(f"  - å”¯ä¸€L1 tokenæ•°é‡: {stats['unique_l1_tokens']}")
            logger.info(f"  - L2åˆ†å¸ƒ: {dict(stats['l2_distribution'])}")
            logger.info(f"  - L3åˆ†å¸ƒ: {dict(stats['l3_distribution'])}")
            logger.info("Trieæ ‘çº¦æŸåˆå§‹åŒ–æˆåŠŸï¼")
            
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–Trieæ ‘çº¦æŸå¤±è´¥: {e}")
            logger.warning("å°†ä½¿ç”¨æ— çº¦æŸç”Ÿæˆæ¨¡å¼")
            self.use_trie_constraint = False
            self.trie = None
            self.constrained_processor = None

    def generate(self, title: str, tags: str = "", max_songs: int = 20, 
                 do_sample: bool = False, num_beams: int = 1, temperature: float = 1.0, 
                 top_k: int = 50, top_p: float = 1.0) -> List[Dict]:
        """
        æ ¹æ®æ ‡é¢˜å’Œæ ‡ç­¾ç”Ÿæˆæ­Œå•ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„æ¨èä¿¡æ¯ã€‚
        
        Args:
            title: æ­Œå•æ ‡é¢˜/æè¿°
            tags: å¯é€‰æ ‡ç­¾
            max_songs: æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: Beam searchæ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: Top-ké‡‡æ ·
            top_p: Top-pé‡‡æ ·
            
        Returns:
            ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸»æ­Œæ›²ã€åŒç°‡æ­Œæ›²ã€è¯­ä¹‰IDå’Œç”Ÿæˆæ¬¡æ•°ç­‰ä¿¡æ¯ã€‚
        """
        prompt = title
        logger.info(f"æ­£åœ¨ç”Ÿæˆæ­Œå•ï¼Œæç¤º: '{prompt}'")

        # --- ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç§å­å›ºå®šéšæœºæ€§ä»¥ä¿è¯å¯å¤ç°æ€§ ---
        seed = self.config.seed
        logger.info(f"ä½¿ç”¨å›ºå®šéšæœºç§å­: {seed}")
        torch.manual_seed(seed)
        # --- ç»“æŸ ---

        input_ids = self.model.tokenizer.base_tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=self.config.generator_t5.max_input_length,
            truncation=True
        ).input_ids.to(self.device)

        gen_kwargs = {
            "max_new_tokens": self.config.generator_t5.max_target_length,
            "pad_token_id": self.model.tokenizer.pad_token_id,
            "num_return_sequences": 1,
        }

        # --- Dynamically build generation arguments ---
        if do_sample:
            gen_kwargs["do_sample"] = True
            gen_kwargs["top_k"] = top_k
            gen_kwargs["top_p"] = top_p
            gen_kwargs["temperature"] = temperature
            logger.info(f"ä½¿ç”¨é‡‡æ ·è§£ç ç­–ç•¥ (Top-K: {top_k}, Top-P: {top_p}, Temp: {temperature})")
        else:
            gen_kwargs["do_sample"] = False
            gen_kwargs["num_beams"] = num_beams
            if num_beams > 1:
                logger.info(f"ä½¿ç”¨Beam Searchè§£ç ç­–ç•¥ (Beams: {num_beams})")
            else:
                logger.info("ä½¿ç”¨Greedy Searchè§£ç ç­–ç•¥")
        
        # æ·»åŠ Trieæ ‘çº¦æŸï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_trie_constraint and self.constrained_processor is not None:
            gen_kwargs["logits_processor"] = [self.constrained_processor]
            logger.info(f"ä½¿ç”¨Trieæ ‘çº¦æŸ")
        else:
            logger.info(f"ä½¿ç”¨æ— çº¦æŸç”Ÿæˆ")

        with torch.no_grad():
            generated_ids = self.model.model.generate(input_ids, **gen_kwargs)
        
        decoded_tokens = self.model.tokenizer.base_tokenizer.convert_ids_to_tokens(
            generated_ids[0], 
            skip_special_tokens=False
        )
        
        logger.debug(f"ç”Ÿæˆçš„token (å‰50ä¸ª): {decoded_tokens[:50]}...")

        semantic_id_tuples = []
        i = 0
        while i < len(decoded_tokens):
            token = decoded_tokens[i]
            if token.startswith("<id_l1_"):
                if i + 2 < len(decoded_tokens):
                    l1_token, l2_token, l3_token = decoded_tokens[i], decoded_tokens[i+1], decoded_tokens[i+2]
                    if (l1_token.startswith("<id_l1_") and 
                        l2_token.startswith("<id_l2_") and 
                        l3_token.startswith("<id_l3_")):
                        try:
                            l1_id = int(l1_token.split('_')[2].rstrip('>'))
                            l2_id = int(l2_token.split('_')[2].rstrip('>'))
                            l3_id = int(l3_token.split('_')[2].rstrip('>'))
                            semantic_id_tuples.append((l1_id, l2_id, l3_id))
                            i += 3
                            continue
                        except (ValueError, IndexError):
                            pass
            i += 1
        
        logger.info(f"æå–äº† {len(semantic_id_tuples)} ä¸ªè¯­ä¹‰IDå…ƒç»„ (åŒ…å«é‡å¤)")
        
        # å¦‚æœä½¿ç”¨äº†Trieæ ‘çº¦æŸï¼ŒéªŒè¯ç”Ÿæˆçš„è¯­ä¹‰IDæ˜¯å¦éƒ½æœ‰æ•ˆ
        if self.use_trie_constraint and self.trie is not None:
            invalid_count = 0
            for id_tuple in semantic_id_tuples:
                if id_tuple not in self.trie.valid_semantic_ids:
                    invalid_count += 1
                    logger.debug(f"æ£€æµ‹åˆ°æ— æ•ˆçš„è¯­ä¹‰ID: {id_tuple}")
            
            if invalid_count > 0:
                logger.warning(f"ç”Ÿæˆäº† {invalid_count} ä¸ªæ— æ•ˆçš„è¯­ä¹‰IDï¼ˆå…±{len(semantic_id_tuples)}ä¸ªï¼‰")
            else:
                logger.info(f"æ‰€æœ‰ç”Ÿæˆçš„è¯­ä¹‰IDéƒ½æ˜¯æœ‰æ•ˆçš„ï¼")

        id_stats = {}
        for i, id_tuple in enumerate(semantic_id_tuples):
            if id_tuple not in id_stats:
                id_stats[id_tuple] = {"count": 1, "first_index": i}
            else:
                id_stats[id_tuple]["count"] += 1
        
        sorted_stats = sorted(
            id_stats.items(), 
            key=lambda item: (-item[1]['count'], item[1]['first_index'])
        )
        
        logger.debug("--- [DEBUG] æ’åºåçš„è¯­ä¹‰IDç”Ÿæˆæ¬¡æ•° (Top 10) ---")
        for id_tuple, stats in sorted_stats[:10]:
            logger.debug(f"ID: {id_tuple}, ç”Ÿæˆæ¬¡æ•°: {stats['count']}, é¦–æ¬¡å‡ºç°ä½ç½®: {stats['first_index']}")
        logger.debug("-------------------------------------------")

        results = []
        for id_tuple, stats in sorted_stats:
            if id_tuple in self.semantic_to_song_cluster:
                song_cluster = self.semantic_to_song_cluster[id_tuple]
                
                sorted_cluster = sorted(song_cluster)
                primary_song_id = sorted_cluster[0]
                similar_song_ids = sorted_cluster[1:6]

                primary_song_info = self.song_info_map.get(primary_song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                similar_songs_info = [
                    {"id": song_id, "info": self.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})}
                    for song_id in similar_song_ids
                ]

                results.append({
                    "primary_song_id": primary_song_id,
                    "primary_song_info": primary_song_info,
                    "semantic_id": id_tuple,
                    "cluster_size": len(song_cluster),
                    "generation_count": stats['count'],
                    "similar_songs": similar_songs_info
                })

                if len(results) >= max_songs:
                    break
            else:
                logger.debug(f"è¯­ä¹‰ID {id_tuple} åœ¨ç°‡æ˜ å°„ä¸­æœªæ‰¾åˆ°")
        
        logger.info(f"æ„å»ºäº† {len(results)} æ¡ç»“æ„åŒ–æ¨èç»“æœ")
        return results

    def _format_song_string(self, song_id: str, song_info: dict) -> str:
        """è¾…åŠ©å‡½æ•°ï¼Œæ ¼å¼åŒ–å•æ›²çš„è¾“å‡ºå­—ç¬¦ä¸²"""
        name = song_info.get("name", "æœªçŸ¥æ­Œæ›²")
        singer = song_info.get("singer", "æœªçŸ¥æ­Œæ‰‹")
        return f"{song_id}-{name}-{singer}"

    def interactive_demo(self, **kwargs):
        """
        å¯åŠ¨äº¤äº’å¼å‘½ä»¤è¡Œæ¼”ç¤ºã€‚
        ä¼šä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„è§£ç å‚æ•°ã€‚
        """
        print("\n" + "="*80)
        print("  ğŸµ T5æ­Œå•ç”Ÿæˆæ¨¡å‹ - äº¤äº’å¼æ¼”ç¤º ğŸµ")
        print("="*80)
        constraint_mode = "Trieæ ‘çº¦æŸ" if self.use_trie_constraint else "æ— çº¦æŸ"
        
        # ä»kwargsè·å–è§£ç è®¾ç½®ç”¨äºæ˜¾ç¤º
        do_sample = kwargs.get('do_sample', False)
        if do_sample:
            decode_strategy = f"é‡‡æ · (Temp: {kwargs.get('temperature', 1.0)}, Top-K: {kwargs.get('top_k', 50)}, Top-P: {kwargs.get('top_p', 1.0)})"
        else:
            num_beams = kwargs.get('num_beams', 1)
            decode_strategy = f"Beam Search (Beams: {num_beams})" if num_beams > 1 else "Greedy Search"

        print(f"  æ¨ç†æ¨¡å¼: {constraint_mode} + {decode_strategy}")
        print("  å‘½ä»¤: 'exit' æˆ– 'quit' é€€å‡ºç¨‹åº")
        print("-"*80)

        while True:
            try:
                prompt = input("\nè¯·è¾“å…¥æ­Œå•æ ‡é¢˜/æè¿° > ").strip()
                if prompt.lower() in ['exit', 'quit']:
                    print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                    break

                if not prompt: 
                    continue

                print("\nğŸ¼ ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...")
                results = self.generate(prompt, **kwargs)

                if not results:
                    print("âŒ æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œæ›²åˆ—è¡¨ï¼Œè¯·å°è¯•æ›´æ¢æ ‡é¢˜æˆ–æè¿°ã€‚")
                    continue
                
                print(f"\nâœ¨ ä¸ºæ‚¨æ¨èçš„æ­Œå• (å…±{len(results)}é¦–): âœ¨")
                print("-"*80)
                for i, item in enumerate(results, 1):
                    primary_str = self._format_song_string(item['primary_song_id'], item['primary_song_info'])
                    
                    similar_list = [self._format_song_string(s['id'], s['info']) for s in item['similar_songs']]
                    similar_str = "; ".join(similar_list)
                    
                    line = f"{i:2d}. {str(item['semantic_id']):<18} - {primary_str}"
                    if similar_str:
                        line += f" ({similar_str})"
                    print(line)
                print("-"*80)

            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
                print(f"\nâŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    def _get_sem_id_for_song(self, song_id_to_find: str) -> Tuple[int, ...]:
        """è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾ç»™å®šæ­Œæ›²IDçš„è¯­ä¹‰IDï¼ˆç”¨äºæ˜¾ç¤ºï¼‰"""
        for sem_id, song_list in self.semantic_to_song_cluster.items():
            if song_id_to_find in song_list:
                return sem_id
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "-m", "--model_path", 
        type=str, 
        default=None,
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: models/generator/final_model)"
    )
    parser.add_argument(
        "-p", "--prompt", 
        type=str, 
        default=None,
        help="ç›´æ¥ç”Ÿæˆæ­Œå•çš„æç¤ºæ–‡æœ¬ (ä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼)"
    )
    parser.add_argument(
        "--max_songs", 
        type=int, 
        default=20,
        help="æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡ (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "-l", "--log_level", 
        type=str, 
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    parser.add_argument(
        "--no_trie_constraint",
        action="store_true",
        help="ç¦ç”¨Trieæ ‘çº¦æŸç”Ÿæˆï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )

    # --- New Decoding Strategy Arguments ---
    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="å¯ç”¨é‡‡æ ·æ¨¡å¼ (é»˜è®¤ç¦ç”¨ï¼Œä¸beam searchäº’æ–¥)"
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=1,
        help="Beam searchçš„beamæ•°é‡ (é»˜è®¤: 1, è¡¨ç¤ºGreedy Search)"
    )
    parser.add_argument(
        "-t", "--temperature", 
        type=float, 
        default=1.0,
        help="é‡‡æ ·æ¸©åº¦ (ä»…åœ¨--do_sampleæ—¶ç”Ÿæ•ˆ, é»˜è®¤: 1.0)"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=50,
        help="Top-ké‡‡æ · (ä»…åœ¨--do_sampleæ—¶ç”Ÿæ•ˆ, é»˜è®¤: 50)"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=1.0,
        help="Top-p (nucleus)é‡‡æ · (ä»…åœ¨--do_sampleæ—¶ç”Ÿæ•ˆ, é»˜è®¤: 1.0)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level)
    setup_logging(level=log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨...")
    use_trie = not args.no_trie_constraint
    generator = PlaylistGenerator(config, model_path=args.model_path, use_trie_constraint=use_trie)
    
    # ç”Ÿæˆæˆ–å¯åŠ¨äº¤äº’æ¨¡å¼
    if args.prompt:
        # å•æ¬¡ç”Ÿæˆæ¨¡å¼
        logger.info(f"æ­£åœ¨ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆæ­Œå•: '{args.prompt}'")
        results = generator.generate(
            args.prompt, 
            max_songs=args.max_songs,
            do_sample=args.do_sample,
            num_beams=args.num_beams,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )
        
        if results:
            print(f"\nç”Ÿæˆçš„æ­Œå• (å…±{len(results)}é¦–):")
            print("="*80)
            for i, item in enumerate(results, 1):
                primary_str = generator._format_song_string(item['primary_song_id'], item['primary_song_info'])
                
                similar_list = [generator._format_song_string(s['id'], s['info']) for s in item['similar_songs']]
                similar_str = "; ".join(similar_list)
                
                line = f"{i:2d}. {str(item['semantic_id']):<18} - {primary_str}"
                if similar_str:
                    line += f" ({similar_str})"
                print(line)
            print("="*80)
        else:
            print("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œå•ï¼Œè¯·å°è¯•å…¶ä»–æç¤ºæ–‡æœ¬ã€‚")
    else:
        # äº¤äº’æ¨¡å¼ - æ³¨æ„ï¼šäº¤äº’æ¨¡å¼å°†ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„è§£ç å‚æ•°
        logger.info("å¯åŠ¨äº¤äº’æ¨¡å¼...")
        logger.info(f"äº¤äº’ä¼šè¯è§£ç å‚æ•°: do_sample={args.do_sample}, num_beams={args.num_beams}, temp={args.temperature}, top_k={args.top_k}, top_p={args.top_p}")
        generator.interactive_demo(
            do_sample=args.do_sample,
            num_beams=args.num_beams,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )


    
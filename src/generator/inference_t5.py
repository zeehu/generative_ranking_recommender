"""
"""
T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†è„šæœ¬
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„T5æ¨¡å‹å¹¶æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ­Œæ›²æ¨è
"""
import os
import sys
import torch
import json
import logging
import random
import argparse
from typing import Dict, Tuple, List
from collections import defaultdict

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import Config
from src.generator.tiger_model import TIGERModel
from src.common.utils import setup_logging

logger = logging.getLogger(__name__)


class PlaylistGenerator:
    """å¤„ç†æ¨¡å‹åŠ è½½å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ­Œå•"""

    def __init__(self, config: Config, model_path: str = None):
        """
        åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if model_path is None:
            model_path = os.path.join(self.config.model_dir, "generator", "final_model")
        self.model_path = model_path
        
        self.model = self._load_model()
        self.semantic_to_song_cluster = self._create_reverse_map()
        self.song_info_map = self._load_song_info()
        self.interactive_deterministic_mode = True # äº¤äº’æ¨¡å¼é»˜è®¤ä¸ºç¡®å®šæ€§

    def _load_model(self) -> TIGERModel:
        """
        æ™ºèƒ½åŠ è½½TIGERæ¨¡å‹ã€‚
        - å¦‚æœæ˜¯æœ€ç»ˆæ¨¡å‹ç›®å½•ï¼Œåˆ™ä½¿ç”¨ TIGERModel.from_pretrainedã€‚
        - å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ç›®å½•ï¼Œåˆ™é€šè¿‡ TIGERModel.__init__ åŠ è½½ã€‚
        """
        if not os.path.exists(self.model_path):
            logger.error(f"é”™è¯¯: æ¨¡å‹æœªæ‰¾åˆ° {self.model_path}")
            logger.error("è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®")
            sys.exit(1)

        is_checkpoint = "checkpoint" in os.path.basename(os.path.normpath(self.model_path))

        try:
            if is_checkpoint:
                logger.info(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ç›®å½•ï¼Œä½¿ç”¨ __init__ æ–¹æ³•åŠ è½½: {self.model_path}")
                rq_config = self.config.h_rqkmeans
                layer_vocab_sizes = {
                    'l1': rq_config.need_clusters[0],
                    'l2': rq_config.need_clusters[1],
                    'l3': rq_config.need_clusters[2],
                }
                model = TIGERModel(base_model=self.model_path, layer_vocab_sizes=layer_vocab_sizes)
            else:
                logger.info(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨ from_pretrained)...")
                model = TIGERModel.from_pretrained(self.model_path)
            
            model.model.to(self.device)
            model.model.eval()
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸã€‚è¯æ±‡è¡¨å¤§å°: {len(model.tokenizer)}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            if is_checkpoint:
                logger.error("åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ã€‚è¯·ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å®Œæ•´ï¼Œå¹¶ä¸”Hugging Faceæ¨¡å‹æ–‡ä»¶å­˜åœ¨ã€‚")
            else:
                logger.error("åŠ è½½æœ€ç»ˆæ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®ä¿æ¨¡å‹æ˜¯ä½¿ç”¨ TIGERModel.save_pretrained ä¿å­˜çš„ã€‚")
            sys.exit(1)

    def _create_reverse_map(self) -> Dict[Tuple[int, ...], List[str]]:
        """åˆ›å»ºä»è¯­ä¹‰IDåˆ°æ­Œæ›²IDåˆ—è¡¨çš„åå‘æ˜ å°„"""
        mapping = defaultdict(list)
        semantic_ids_file = os.path.join(self.config.output_dir, "semantic_id", "song_semantic_ids.jsonl")
        if not os.path.exists(semantic_ids_file):
            logger.error(f"é”™è¯¯: song_semantic_ids.jsonl æœªæ‰¾åˆ°äº {semantic_ids_file}")
            logger.error("è¯·å…ˆè¿è¡Œè¯­ä¹‰IDç”Ÿæˆæ­¥éª¤")
            sys.exit(1)

        logger.info("æ­£åœ¨åˆ›å»ºè¯­ä¹‰IDåˆ°æ­Œæ›²ç°‡çš„åå‘æ˜ å°„...")
        with open(semantic_ids_file, 'r', encoding='utf-8') as f:
            for line in f: 
                item = json.loads(line)
                mapping[tuple(item['semantic_ids'])].append(item['song_id'])
        logger.info(f"å·²åŠ è½½ {len(mapping)} ä¸ªå”¯ä¸€çš„è¯­ä¹‰IDç°‡")
        return mapping

    def _load_song_info(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½æ­Œæ›²ä¿¡æ¯ï¼ˆæ­Œåã€æ­Œæ‰‹ï¼‰"""
        import csv
        mapping = {}
        try:
            with open(self.config.data.song_info_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f, delimiter='\t')
                for row in reader:
                    if len(row) >= 3: 
                        mapping[row[0]] = {"name": row[1], "singer": row[2]}
            logger.info(f"å·²åŠ è½½ {len(mapping)} é¦–æ­Œæ›²çš„ä¿¡æ¯")
        except FileNotFoundError: 
            logger.warning(f"æ­Œæ›²ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°: {self.config.data.song_info_file}")
        return mapping

    def generate(self, title: str, tags: str = "", max_songs: int = 20, temperature: float = 0.8, deterministic: bool = True, num_beams: int = 5) -> List[Dict]:
        """
        æ ¹æ®æ ‡é¢˜å’Œæ ‡ç­¾ç”Ÿæˆæ­Œå•ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„æ¨èä¿¡æ¯ã€‚
        
        Args:
            title: æ­Œå•æ ‡é¢˜/æè¿°
            tags: å¯é€‰æ ‡ç­¾
            max_songs: æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            deterministic: æ˜¯å¦ç¡®å®šæ€§æ¨ç†
            num_beams: æŸæœç´¢å¤§å°
            
        Returns:
            ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸»æ­Œæ›²ã€åŒç°‡æ­Œæ›²ã€è¯­ä¹‰IDå’Œç”Ÿæˆæ¬¡æ•°ç­‰ä¿¡æ¯ã€‚
        """
        prompt = title
        logger.info(f"æ­£åœ¨ç”Ÿæˆæ­Œå•ï¼Œæç¤º: '{prompt}'")

        input_ids = self.model.tokenizer.base_tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=self.config.generator_t5.max_input_length,
            truncation=True
        ).input_ids.to(self.device)

        gen_kwargs = {
            "max_new_tokens": self.config.generator_t5.max_target_length,
            "pad_token_id": self.model.tokenizer.pad_token_id,
            "num_return_sequences": 1,
        }

        if deterministic:
            logger.info(f"ä½¿ç”¨ç¡®å®šæ€§æ¨ç† (Beam Search, num_beams={num_beams})")
            gen_kwargs['do_sample'] = False
            gen_kwargs['num_beams'] = num_beams
        else:
            logger.info(f"ä½¿ç”¨é‡‡æ ·æ¨ç† (Sampling, temperature={temperature})")
            gen_kwargs['do_sample'] = True
            gen_kwargs['top_k'] = 50
            gen_kwargs['top_p'] = 0.95
            gen_kwargs['temperature'] = temperature

        with torch.no_grad():
            generated_ids = self.model.model.generate(input_ids, **gen_kwargs)
        
        decoded_tokens = self.model.tokenizer.base_tokenizer.convert_ids_to_tokens(
            generated_ids[0], 
            skip_special_tokens=False
        )
        
        logger.debug(f"ç”Ÿæˆçš„token (å‰50ä¸ª): {decoded_tokens[:50]}...")

        semantic_id_tuples = []
        i = 0
        while i < len(decoded_tokens):
            token = decoded_tokens[i]
            if token.startswith("<id_l1_"):
                if i + 2 < len(decoded_tokens):
                    l1_token, l2_token, l3_token = decoded_tokens[i], decoded_tokens[i+1], decoded_tokens[i+2]
                    if (l1_token.startswith("<id_l1_") and 
                        l2_token.startswith("<id_l2_") and 
                        l3_token.startswith("<id_l3_")):
                        try:
                            l1_id = int(l1_token.split('_')[2].rstrip('>'))
                            l2_id = int(l2_token.split('_')[2].rstrip('>'))
                            l3_id = int(l3_token.split('_')[2].rstrip('>'))
                            semantic_id_tuples.append((l1_id, l2_id, l3_id))
                            i += 3
                            continue
                        except (ValueError, IndexError):
                            pass
            i += 1
        
        logger.info(f"æå–äº† {len(semantic_id_tuples)} ä¸ªè¯­ä¹‰IDå…ƒç»„ (åŒ…å«é‡å¤)")

        id_stats = {}
        for i, id_tuple in enumerate(semantic_id_tuples):
            if id_tuple not in id_stats:
                id_stats[id_tuple] = {"count": 1, "first_index": i}
            else:
                id_stats[id_tuple]["count"] += 1
        
        sorted_stats = sorted(
            id_stats.items(), 
            key=lambda item: (-item[1]['count'], item[1]['first_index'])
        )
        
        logger.debug("--- [DEBUG] æ’åºåçš„è¯­ä¹‰IDç”Ÿæˆæ¬¡æ•° (Top 10) ---")
        for id_tuple, stats in sorted_stats[:10]:
            logger.debug(f"ID: {id_tuple}, ç”Ÿæˆæ¬¡æ•°: {stats['count']}, é¦–æ¬¡å‡ºç°ä½ç½®: {stats['first_index']}")
        logger.debug("-------------------------------------------")

        results = []
        for id_tuple, stats in sorted_stats:
            if id_tuple in self.semantic_to_song_cluster:
                song_cluster = self.semantic_to_song_cluster[id_tuple]
                
                sorted_cluster = sorted(song_cluster)
                primary_song_id = sorted_cluster[0]
                similar_song_ids = sorted_cluster[1:6]

                primary_song_info = self.song_info_map.get(primary_song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})
                similar_songs_info = [
                    {"id": song_id, "info": self.song_info_map.get(song_id, {"name": "æœªçŸ¥æ­Œæ›²", "singer": "æœªçŸ¥æ­Œæ‰‹"})}
                    for song_id in similar_song_ids
                ]

                results.append({
                    "primary_song_id": primary_song_id,
                    "primary_song_info": primary_song_info,
                    "semantic_id": id_tuple,
                    "cluster_size": len(song_cluster),
                    "generation_count": stats['count'],
                    "similar_songs": similar_songs_info
                })

                if len(results) >= max_songs:
                    break
            else:
                logger.debug(f"è¯­ä¹‰ID {id_tuple} åœ¨ç°‡æ˜ å°„ä¸­æœªæ‰¾åˆ°")
        
        logger.info(f"æ„å»ºäº† {len(results)} æ¡ç»“æ„åŒ–æ¨èç»“æœ")
        return results

    def _format_song_string(self, song_id: str, song_info: dict) -> str:
        """è¾…åŠ©å‡½æ•°ï¼Œæ ¼å¼åŒ–å•æ›²çš„è¾“å‡ºå­—ç¬¦ä¸²"""
        name = song_info.get("name", "æœªçŸ¥æ­Œæ›²")
        singer = song_info.get("singer", "æœªçŸ¥æ­Œæ‰‹")
        return f"{song_id}-{name}-{singer}"

    def interactive_demo(self):
        """å¯åŠ¨äº¤äº’å¼å‘½ä»¤è¡Œæ¼”ç¤º"""
        print("\n" + "="*80)
        print("  ğŸµ T5æ­Œå•ç”Ÿæˆæ¨¡å‹ - äº¤äº’å¼æ¼”ç¤º ğŸµ")
        print("="*80)
        print(f"  å½“å‰æ¨¡å¼: {'ç¡®å®šæ€§' if self.interactive_deterministic_mode else 'å¤šæ ·æ€§é‡‡æ ·'}")
        print("  å‘½ä»¤:")
        print("    - 'set mode det': åˆ‡æ¢åˆ°ç¡®å®šæ€§æ¨¡å¼ (å¯å¤ç°)")
        print("    - 'set mode sample': åˆ‡æ¢åˆ°å¤šæ ·æ€§é‡‡æ ·æ¨¡å¼ (éšæœº)")
        print("    - 'exit' æˆ– 'quit': é€€å‡ºç¨‹åº")
        print("-"*80)

        while True:
            try:
                prompt = input("\nè¯·è¾“å…¥æ­Œå•æ ‡é¢˜/æè¿° > ").strip()
                if prompt.lower() in ['exit', 'quit']:
                    print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                    break
                
                if prompt.lower() == 'set mode det':
                    self.interactive_deterministic_mode = True
                    print(f"âœ… æ¨¡å¼å·²åˆ‡æ¢ä¸º: ç¡®å®šæ€§")
                    continue
                
                if prompt.lower() == 'set mode sample':
                    self.interactive_deterministic_mode = False
                    print(f"âœ… æ¨¡å¼å·²åˆ‡æ¢ä¸º: å¤šæ ·æ€§é‡‡æ ·")
                    continue

                if not prompt: 
                    continue

                print("\nğŸ¼ ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...")
                results = self.generate(prompt, deterministic=self.interactive_deterministic_mode)

                if not results:
                    print("âŒ æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œæ›²åˆ—è¡¨ï¼Œè¯·å°è¯•æ›´æ¢æ ‡é¢˜æˆ–æè¿°ã€‚")
                    continue
                
                print(f"\nâœ¨ ä¸ºæ‚¨æ¨èçš„æ­Œå• (å…±{len(results)}é¦–): âœ¨")
                print("-"*80)
                for i, item in enumerate(results, 1):
                    primary_str = self._format_song_string(item['primary_song_id'], item['primary_song_info'])
                    
                    similar_list = [self._format_song_string(s['id'], s['info']) for s in item['similar_songs']]
                    similar_str = "; ".join(similar_list)
                    
                    line = f"{i:2d}. {str(item['semantic_id']):<18} - {primary_str}"
                    if similar_str:
                        line += f" ({similar_str})"
                    print(line)
                print("-"*80)

            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
                print(f"\nâŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    def _get_sem_id_for_song(self, song_id_to_find: str) -> Tuple[int, ...]:
        """è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾ç»™å®šæ­Œæ›²IDçš„è¯­ä¹‰IDï¼ˆç”¨äºæ˜¾ç¤ºï¼‰"""
        for sem_id, song_list in self.semantic_to_song_cluster.items():
            if song_id_to_find in song_list:
                return sem_id
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T5æ­Œå•ç”Ÿæˆæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "-m", "--model_path", 
        type=str, 
        default=None,
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: models/generator/final_model)"
    )
    parser.add_argument(
        "-p", "--prompt", 
        type=str, 
        default=None,
        help="ç›´æ¥ç”Ÿæˆæ­Œå•çš„æç¤ºæ–‡æœ¬ (ä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼)"
    )
    parser.add_argument(
        "--max_songs", 
        type=int, 
        default=20,
        help="æœ€å¤§ç”Ÿæˆæ­Œæ›²æ•°é‡ (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "-t", "--temperature", 
        type=float, 
        default=0.8,
        help="é‡‡æ ·æ¸©åº¦ï¼Œä»…åœ¨é‡‡æ ·æ¨¡å¼ä¸‹æœ‰æ•ˆ (é»˜è®¤: 0.8)"
    )
    parser.add_argument(
        "-l", "--log_level", 
        type=str, 
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    parser.add_argument(
        "-s", "--sample",
        action="store_true",
        help="ä½¿ç”¨é‡‡æ ·æ¨ç†ï¼ˆéšæœºæ¨¡å¼ï¼‰ï¼Œé»˜è®¤ä¸ºç¡®å®šæ€§æ¨ç†"
    )
    parser.add_argument(
        "-b", "--num_beams",
        type=int,
        default=5,
        help="åœ¨ç¡®å®šæ€§æ¨ç†ä¸­ä½¿ç”¨çš„æŸæ•°é‡ (é»˜è®¤: 5)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level)
    setup_logging(level=log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ­Œå•ç”Ÿæˆå™¨...")
    generator = PlaylistGenerator(config, model_path=args.model_path)
    
    # ç”Ÿæˆæˆ–å¯åŠ¨äº¤äº’æ¨¡å¼
    if args.prompt:
        # å•æ¬¡ç”Ÿæˆæ¨¡å¼
        is_deterministic = not args.sample
        logger.info(f"æ­£åœ¨ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆæ­Œå•: '{args.prompt}'")
        results = generator.generate(
            args.prompt, 
            max_songs=args.max_songs,
            temperature=args.temperature,
            deterministic=is_deterministic,
            num_beams=args.num_beams
        )
        
        if results:
            print(f"\nç”Ÿæˆçš„æ­Œå• (å…±{len(results)}é¦–):")
            print("="*80)
            for i, item in enumerate(results, 1):
                primary_str = generator._format_song_string(item['primary_song_id'], item['primary_song_info'])
                
                similar_list = [generator._format_song_string(s['id'], s['info']) for s in item['similar_songs']]
                similar_str = "; ".join(similar_list)
                
                line = f"{i:2d}. {str(item['semantic_id']):<18} - {primary_str}"
                if similar_str:
                    line += f" ({similar_str})"
                print(line)
            print("="*80)
        else:
            print("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ­Œå•ï¼Œè¯·å°è¯•å…¶ä»–æç¤ºæ–‡æœ¬ã€‚")
    else:
        # äº¤äº’æ¨¡å¼
        generator.interactive_demo()